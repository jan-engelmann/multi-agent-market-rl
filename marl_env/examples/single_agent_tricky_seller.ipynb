{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from stable_baselines import DQN\n",
    "from stable_baselines.common.vec_env import DummyVecEnv\n",
    "\n",
    "from dmarket.environments import SingleAgentTrainingEnv\n",
    "from dmarket.agents import UniformRandomAgent, GymRLAgent, TimeLinearAgent, TimeDependentAgent\n",
    "from dmarket.info_settings import OfferInformationSetting, TimeInformationWrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a simple memory-based seller. The seller offers in the interval ``[50, 100]`` and has the following strategy. If he sees that someone is bidding above 90, he will increase ``count`` by one in his internal memory. He will then offer ``100 - count``. If, however, during some round ``count >= 3``, then he will throw a large discount and offer to sell at 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrickySeller(TimeDependentAgent):\n",
    "    def compute_offer(self, obs, time):\n",
    "        if time == 0:\n",
    "            self.count = 0\n",
    "            \n",
    "        if self.count >= 3:\n",
    "            # DISCOUNT TIME!!!\n",
    "            return 50\n",
    "        else:\n",
    "            best_bid = obs[0][0]\n",
    "            if best_bid >= 90:\n",
    "                self.count += 1\n",
    "            return 100 - self.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up environment for baselines training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_agent = GymRLAgent('buyer', 100, 'B1', max_factor=0.5, discretization=10)\n",
    "\n",
    "fixed_agents = [\n",
    "    TrickySeller('seller', 50,  'TS')\n",
    "]\n",
    "\n",
    "setting = TimeInformationWrapper(OfferInformationSetting(1))\n",
    "\n",
    "def get_env(rl_agent, fixed_agents, setting):\n",
    "    return SingleAgentTrainingEnv(rl_agent, fixed_agents, setting)\n",
    "\n",
    "env = DummyVecEnv([lambda: get_env(rl_agent, fixed_agents, setting)]) # wrap it for baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the RL-agent has a discrete action space (integers from 0 to 9 inclusive), let's see what prices RL agent can offer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 100.0),\n",
       " (1, 95.0),\n",
       " (2, 90.0),\n",
       " (3, 85.0),\n",
       " (4, 80.0),\n",
       " (5, 75.0),\n",
       " (6, 70.0),\n",
       " (7, 65.0),\n",
       " (8, 60.0),\n",
       " (9, 55.0)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(action, rl_agent.action_to_price(action)) for action in range(rl_agent.discretization)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, the maximal reward the agent can get is 47.5. To do this, he needs to follow the following sequence: offer 3 times 95 to trigger the seller and then offer 55. At the last step, the seller will offer 50, so the midprice would be 52.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if the DQN algorithm can learn this simple policy. **Note: the RL agent will not be able to access the time information, only the last best offers**. The seller will signal how long until it will drop the price by 50.\n",
    "\n",
    "We will set the algorithm to do a lot of exploration: anywhere between 30% to 50% of the time it will do something random. The discount rate is set to 90%, so that the agent is pushed to quickly trigger the correct sequence to obtain the reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
   ],
   "source": [
    "model = DQN(\"LnMlpPolicy\", env, verbose=1, \n",
    "            gamma=0.90, exploration_fraction=0.5,\n",
    "            exploration_final_eps=0.3,\n",
    "            prioritized_replay=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learn for 10000 steps, which should take about a minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "| % time spent exploring  | 89       |\n",
      "| episodes                | 100      |\n",
      "| mean 100 episode reward | 11.6     |\n",
      "| steps                   | 730      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 76       |\n",
      "| episodes                | 200      |\n",
      "| mean 100 episode reward | 10.5     |\n",
      "| steps                   | 1643     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 66       |\n",
      "| episodes                | 300      |\n",
      "| mean 100 episode reward | 19.8     |\n",
      "| steps                   | 2414     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 56       |\n",
      "| episodes                | 400      |\n",
      "| mean 100 episode reward | 24.7     |\n",
      "| steps                   | 3112     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 47       |\n",
      "| episodes                | 500      |\n",
      "| mean 100 episode reward | 25.9     |\n",
      "| steps                   | 3725     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 39       |\n",
      "| episodes                | 600      |\n",
      "| mean 100 episode reward | 32.5     |\n",
      "| steps                   | 4305     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 31       |\n",
      "| episodes                | 700      |\n",
      "| mean 100 episode reward | 35.5     |\n",
      "| steps                   | 4859     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 30       |\n",
      "| episodes                | 800      |\n",
      "| mean 100 episode reward | 40.5     |\n",
      "| steps                   | 5414     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 30       |\n",
      "| episodes                | 900      |\n",
      "| mean 100 episode reward | 38.7     |\n",
      "| steps                   | 5945     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 30       |\n",
      "| episodes                | 1000     |\n",
      "| mean 100 episode reward | 38.7     |\n",
      "| steps                   | 6497     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 30       |\n",
      "| episodes                | 1100     |\n",
      "| mean 100 episode reward | 37.2     |\n",
      "| steps                   | 7040     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 30       |\n",
      "| episodes                | 1200     |\n",
      "| mean 100 episode reward | 40       |\n",
      "| steps                   | 7587     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 30       |\n",
      "| episodes                | 1300     |\n",
      "| mean 100 episode reward | 37.4     |\n",
      "| steps                   | 8128     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 30       |\n",
      "| episodes                | 1400     |\n",
      "| mean 100 episode reward | 38.9     |\n",
      "| steps                   | 8649     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 30       |\n",
      "| episodes                | 1500     |\n",
      "| mean 100 episode reward | 38.3     |\n",
      "| steps                   | 9196     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 30       |\n",
      "| episodes                | 1600     |\n",
      "| mean 100 episode reward | 38.7     |\n",
      "| steps                   | 9732     |\n",
      "--------------------------------------\n",
      "CPU times: user 1min 47s, sys: 4.6 s, total: 1min 52s\n",
      "Wall time: 1min 9s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines.deepq.dqn.DQN at 0x7f9c04c49850>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model.learn(total_timesteps=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the agent's policy in a probability heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_agent.model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_transition(agent):\n",
    "    data = []\n",
    "    for price in range(100, 50, -1):\n",
    "        data.append(agent.model.action_probability(\n",
    "            rl_agent.normalize(np.array([\n",
    "                [0],\n",
    "                [price],\n",
    "            ])\n",
    "        )))\n",
    "    \n",
    "    min_offer = agent._a\n",
    "    max_offer = agent._b\n",
    "    plt.imshow(np.array(data), extent=[max_offer, min_offer, min_offer, max_offer])\n",
    "    plt.xlabel('Next offer probability')\n",
    "    plt.ylabel('Best last ask')\n",
    "    plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_transition(rl_agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the agent has succesfully learned to exploit the situation! In the graph, if the seller is asking for something close 100, the RL buyer will likely offer a high price to trigger the seller."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "abm",
   "language": "python",
   "name": "abm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
