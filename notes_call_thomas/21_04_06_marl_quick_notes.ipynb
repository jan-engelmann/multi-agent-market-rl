{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "flying-minute",
   "metadata": {},
   "source": [
    "# Quick Notes on MARL\n",
    "Here we evaluate some concepts for MARL.\n",
    "\n",
    "## General Class Structure for Hierarchical Actions\n",
    "\n",
    "Hierarchical actions: decisions taken in sequence. Often actions are of mixed type, e.g. both binary and continue.\n",
    "\n",
    "Goal is to reduce the action space, i.e. constrain the model to evaluate action values that would make sense in real world.\n",
    "See the abstract structure below.\n",
    "Ideally hierarchical actions should have decoupled rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "iraqi-irish",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def somefunc(*args):\n",
    "    pass\n",
    "class Agent:\n",
    "    def __init__(self, decision_policy, offer_polcy):\n",
    "        pass\n",
    "        \n",
    "    def act(self, observations):\n",
    "        decision = torch.rand_like(observations) @ observations\n",
    "        return decision\n",
    "        \n",
    "\n",
    "\n",
    "class PolicyBinaryDecision(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def act(self, observations):\n",
    "        actions = torch.zeros_like(observations)\n",
    "        return actions\n",
    "\n",
    "class PolicyOffer(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, observations):\n",
    "        actions = torch.zeros_like(observations)\n",
    "        return actions\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "class Environment():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def step(self, actions):\n",
    "        decision = actions[0]\n",
    "        offer_value = actions[1:]\n",
    "        \n",
    "        reward_binary = somefunc(decision)\n",
    "        reward_offer = somefunc(offer_value)\n",
    "        \n",
    "        reward_binary = reward_binary if reward_binary > reward_offer else - reward_offer \n",
    "        \n",
    "        return observations, (reward_binary, reward_offer), info\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "connected-skill",
   "metadata": {},
   "source": [
    "## Composite Actions\n",
    "\n",
    "In case the previous proves too computationally, one can force the constraints inside the policy neural network forward or the environment by calculating a composite action that combines both decision and reward. \n",
    "Then the reward would not be decoupled for actions, but still the neural network will evaluate valid combinations of actions, e.g. when passing offer price will be always 0.\n",
    "An abstract example of such composite action policy is found below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "attractive-independence",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_action():\n",
    "    logits = nn_policy_logits() #batch_size x 1\n",
    "    decision_prob = torch.sigmoid(logits) # (0,1)\n",
    "    \n",
    "    price = torch.relu(nn_quantity_output(...) - reservation) + reservation# seller\n",
    "    # price = - torch.relu(reservation - nn_quantity_output(...)) + reservation# buyer\n",
    "\n",
    "    return torch.bernoulli(p=decision_prob)*price\n",
    "\n",
    "# but\n",
    "composite_action = torch.rand([5, 1]) # a continuous composite action generated by policy action\n",
    "reward = somefunc(composite_action)\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "short-driving",
   "metadata": {},
   "source": [
    "If we use the composite action logic and not decouple the reward, we reduce the action space but may still require high sample counts. The RL will require a lot of samples to \"learn\" and decouple the reward values and assign credit to the proper actions.\n",
    "In RL this is commonly known as the credit assignment problem, and is a major challenge in reward design."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pregnant-thanksgiving",
   "metadata": {},
   "source": [
    "## Centralized Environment and Decentrallized Agents\n",
    "Here we evaluate how it is possible with one environment to parellized and handle many samples (mini-batches, agents and timesteps)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stock-blast",
   "metadata": {},
   "source": [
    "Does having a centrallized shared environment allow for decentralized agent architectures? \n",
    "For this we come up with simple small example of 3 agents playing the following game:\n",
    "- Each agent outputs a continuous value action.\n",
    "- The agent that outputs the median value wins for the given timestep.\n",
    "- The agent that wins most timesteps, wins the game\n",
    "\n",
    "From RL perspective the agents can learn by interacting with several instances of the same environment per time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "amber-impression",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we had 3 agents that are \"linear regressors\", and each agent takes the mean over other agent's actions as an observation\n",
    "\n",
    "n_samples = 6\n",
    "n_agents = 3\n",
    "\n",
    "\n",
    "agent_adjacency = torch.tensor([\n",
    "    [0, 1, 1],\n",
    "    [1, 0, 1],\n",
    "    [1, 1, 0]\n",
    "], dtype=torch.float)\n",
    "\n",
    "# TODO: There is an error in the observation calcultation. Double check if important\n",
    "class Env:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def step(self, actions): \n",
    "        \"\"\"\n",
    "        param actions: a n_samples x n_agents vector\n",
    "        \"\"\"\n",
    "        median_action = actions.median(-1).values # over all agents not samples\n",
    "\n",
    "        reward = -((actions - median_action.unsqueeze(-1))**2) # we want broadcast over agent dimension\n",
    "        observation = (agent_adjacency.unsqueeze(0)*actions.unsqueeze(-1)).mean(-1) # we match samples and agents dimension for broadcast\n",
    "        \n",
    "        # detach below means we do not allow gradient propagation through environment calculations!\n",
    "        return observation.detach(), reward.detach(), {}\n",
    "    \n",
    "    def reset(self, n_samples = 10): # or we can call it n_environments \n",
    "        return torch.rand([n_samples, n_agents])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beautiful-design",
   "metadata": {},
   "source": [
    "Given the above the environment step would look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "august-playing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[2.0000, 1.3333, 0.6667],\n",
       "         [2.6667, 2.0000, 3.3333]]),\n",
       " tensor([[-1,  0, -1],\n",
       "         [ 0, -1, -1]]),\n",
       " {})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = Env()\n",
    "env.step(torch.tensor(     #agent_1, agent_2, agent_3\n",
    "                           [[       3,       2,       1],  # env 1\n",
    "                            [       4,       3,       5] ] # env 2\n",
    "                ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alike-resident",
   "metadata": {},
   "source": [
    "Now let's play with some agents that do linear regression and optimize a q-value loss over time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "convertible-encounter",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 1\n",
    "\n",
    "agent_1 = torch.nn.Linear(in_features=n_features, out_features=1)\n",
    "agent_2 = torch.nn.Linear(in_features=n_features, out_features=1)\n",
    "agent_3 = torch.nn.Linear(in_features=n_features, out_features=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worst-judgment",
   "metadata": {},
   "source": [
    "We collect initial observations for 10 samples:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "unusual-fluid",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2929, 0.2356, 0.4162],\n",
       "        [0.3818, 0.3747, 0.2764],\n",
       "        [0.2781, 0.2811, 0.5638],\n",
       "        [0.5451, 0.6116, 0.9397],\n",
       "        [0.0962, 0.5979, 0.0512],\n",
       "        [0.9486, 0.3938, 0.1681],\n",
       "        [0.4565, 0.6818, 0.0627],\n",
       "        [0.9360, 0.9369, 0.5536],\n",
       "        [0.8793, 0.7154, 0.0522],\n",
       "        [0.7321, 0.5410, 0.2515]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_observation =  env.reset()\n",
    "initial_observation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accessory-flooring",
   "metadata": {},
   "source": [
    "Then we calculate first step actions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "allied-quest",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4671, -0.9128,  0.3898],\n",
       "        [ 0.3975, -1.0193,  0.3646],\n",
       "        [ 0.4787, -0.9477,  0.4163],\n",
       "        [ 0.2697, -1.2007,  0.4839],\n",
       "        [ 0.6210, -1.1902,  0.3241],\n",
       "        [-0.0462, -1.0340,  0.3451],\n",
       "        [ 0.3390, -1.2544,  0.3262],\n",
       "        [-0.0363, -1.4497,  0.4145],\n",
       "        [ 0.0081, -1.2801,  0.3243],\n",
       "        [ 0.1233, -1.1466,  0.3601]], grad_fn=<CatBackward>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_agent_1 = agent_1.forward(initial_observation[:, 0].unsqueeze(-1))\n",
    "action_agent_2 = agent_2.forward(initial_observation[:, 1].unsqueeze(-1))\n",
    "action_agent_3 = agent_3.forward(initial_observation[:, 2].unsqueeze(-1))\n",
    "all_actions = torch.cat([action_agent_1, action_agent_2, action_agent_3], dim=-1) # concatenate agents on last dimension\n",
    "all_actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simplified-floor",
   "metadata": {},
   "source": [
    "The observations and rewards for the next step would be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cardiac-mileage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.3114, -0.6085,  0.2598],\n",
      "        [ 0.2650, -0.6795,  0.2431],\n",
      "        [ 0.3191, -0.6318,  0.2775],\n",
      "        [ 0.1798, -0.8004,  0.3226],\n",
      "        [ 0.4140, -0.7935,  0.2161],\n",
      "        [-0.0308, -0.6893,  0.2301],\n",
      "        [ 0.2260, -0.8363,  0.2174],\n",
      "        [-0.0242, -0.9665,  0.2763],\n",
      "        [ 0.0054, -0.8534,  0.2162],\n",
      "        [ 0.0822, -0.7644,  0.2401]])\n",
      "tensor([[-5.9737e-03, -1.6967e+00, -0.0000e+00],\n",
      "        [-1.0825e-03, -1.9153e+00, -0.0000e+00],\n",
      "        [-3.8875e-03, -1.8605e+00, -0.0000e+00],\n",
      "        [-0.0000e+00, -2.1619e+00, -4.5906e-02],\n",
      "        [-8.8161e-02, -2.2931e+00, -0.0000e+00],\n",
      "        [-0.0000e+00, -9.7566e-01, -1.5313e-01],\n",
      "        [-1.6445e-04, -2.4983e+00, -0.0000e+00],\n",
      "        [-0.0000e+00, -1.9977e+00, -2.0322e-01],\n",
      "        [-0.0000e+00, -1.6595e+00, -9.9960e-02],\n",
      "        [-0.0000e+00, -1.6128e+00, -5.6075e-02]])\n"
     ]
    }
   ],
   "source": [
    "obs, rew, _ = env.step(all_actions)\n",
    "print(obs) # observation should be n_samples\n",
    "print(rew)  # shape should be n_samples x n_agents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "velvet-wound",
   "metadata": {},
   "source": [
    "To start training we create an optimizer per agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "infrared-prediction",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizers = [torch.optim.Adam(agent_1.parameters()),\n",
    "              torch.optim.Adam(agent_2.parameters()),\n",
    "              torch.optim.Adam(agent_3.parameters())]\n",
    "\n",
    "all_agents = [agent_1, agent_2, agent_3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "explicit-phenomenon",
   "metadata": {},
   "source": [
    "Now if we wanted to backpropagate and our learning loss would be something simple, e.g. discounted sum of rewards times actions, we would end up having the following learning loop in torch.\n",
    "First we sample actions, observations and rewards based on the current agent (neural network) parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "binding-bottle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A game of 100 steps begins\n",
    "obs =  env.reset()\n",
    "\n",
    "# rollout experience collection\n",
    "all_actions = []\n",
    "all_rewards = []\n",
    "all_obs = []\n",
    "for step in range(100):\n",
    "    \n",
    "    actions = torch.cat([agent_1.forward(obs[:, 0].unsqueeze(-1).clone()), \n",
    "                         agent_2.forward(obs[:, 1].unsqueeze(-1).clone()), \n",
    "                         agent_3.forward(obs[:, 2].unsqueeze(-1).clone())\n",
    "                        ],\n",
    "                        -1).clone() # stack agents on last dimension\n",
    "    obs, rew, _ = env.step(actions)\n",
    "    all_actions.append(actions)\n",
    "    all_rewards.append(rew)\n",
    "    all_obs.append(obs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expected-sessions",
   "metadata": {},
   "source": [
    "Then we use these experiences to backpropagate and update parameters.\n",
    "Once we update we can repeat the previous step and create a loop. \n",
    "We perform backpropagation update by rolling out and calculating the learning loss based on the discount rewards!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "czech-calculator",
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a2e45de68863>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.99\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdiscount_coefficients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_obs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0magent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_agents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_obs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0magent_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_agents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "gamma = 0.99\n",
    "discount_coefficients = (gamma)**torch.arange(len(all_obs))\n",
    "for agent in range(n_agents):\n",
    "    for i, obs in enumerate(all_obs):\n",
    "        agent_action = all_agents[agent](obs[:, agent].unsqueeze(-1))\n",
    "        \n",
    "        # here we use a simple loss, q-value and belmann equation could replace it...\n",
    "        loss = (-torch.stack(all_rewards[i:])[:, :, agent].sum(-1)*discount_coefficients[i:]*agent_action**2).sum() # discount factor goes here\n",
    "        loss.backward() # gradient calculation\n",
    "        optimizers[agent].step() # parameter update\n",
    "        print(loss) # we should see this dropiing as we repeat the training loop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "relevant-column",
   "metadata": {},
   "source": [
    "## Extra tips\n",
    "If the agents would receive $n$ features per agent, then the linear layers would receive an input of $n \\times m$, where $m$ is the number of agents. We would have to reshape in torch the observation inputs accordingly.\n",
    "\n",
    "### Broadcasting and dummy dimensions\n",
    "\n",
    "We can use dummy dimensions to apply broadcasting in torch, in a similar fashion to numpy. \n",
    "More about broadcasting semantics can be found here:\n",
    "https://pytorch.org/docs/stable/notes/broadcasting.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python385jvsc74a57bd08cce4a0e23593a4ca2ecb1856e664a4f8d56e21f38038162dc10fe46e54c14c1",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}