
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Extra Material &#8212; multi-agent-market-rl 26.7.21 documentation</title>
    <link rel="stylesheet" type="text/css" href="../../../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/alabaster.css" />
    <script data-url_root="../../../../" id="documentation_options" src="../../../../_static/documentation_options.js"></script>
    <script src="../../../../_static/jquery.js"></script>
    <script src="../../../../_static/underscore.js"></script>
    <script src="../../../../_static/doctools.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" />
   
  <link rel="stylesheet" href="../../../../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <section id="extra-material">
<h1>Extra Material<a class="headerlink" href="#extra-material" title="Permalink to this headline">¶</a></h1>
<section id="proof-for-using-q-function-in-policy-gradient-formula">
<h2>Proof for Using Q-Function in Policy Gradient Formula<a class="headerlink" href="#proof-for-using-q-function-in-policy-gradient-formula" title="Permalink to this headline">¶</a></h2>
<p>In this section, we will show that</p>
<div class="math notranslate nohighlight">
\[\nabla_{\theta} J(\pi_{\theta}) &amp;= \underE{\tau \sim \pi_{\theta}}{\sum_{t=0}^{T} \Big( \nabla_{\theta} \log \pi_{\theta}(a_t |s_t) \Big) Q^{\pi_{\theta}}(s_t, a_t)},\]</div>
<p>for the finite-horizon undiscounted return setting. (An analagous result holds in the infinite-horizon discounted case using basically the same proof.)</p>
<p>The proof of this claim depends on the <a class="reference external" href="https://en.wikipedia.org/wiki/Law_of_total_expectation">law of iterated expectations</a>. First, let’s rewrite the expression for the policy gradient, starting from the reward-to-go form (using the notation <span class="math notranslate nohighlight">\(\hat{R}_t = \sum_{t'=t}^T R(s_t', a_t', s_{t'+1})\)</span> to help shorten things):</p>
<div class="math notranslate nohighlight">
\[\begin{split}\nabla_{\theta} J(\pi_{\theta}) &amp;= \underE{\tau \sim \pi_{\theta}}{\sum_{t=0}^{T} \nabla_{\theta} \log \pi_{\theta}(a_t |s_t) \hat{R}_t} \\
&amp;= \sum_{t=0}^{T} \underE{\tau \sim \pi_{\theta}}{\nabla_{\theta} \log \pi_{\theta}(a_t |s_t) \hat{R}_t}\end{split}\]</div>
<p>Define <span class="math notranslate nohighlight">\(\tau_{:t} = (s_0, a_0, ..., s_t, a_t)\)</span> as the trajectory up to time <span class="math notranslate nohighlight">\(t\)</span>, and <span class="math notranslate nohighlight">\(\tau_{t:}\)</span> as the remainder of the trajectory after that. By the law of iterated expectations, we can break up the preceding expression into:</p>
<div class="math notranslate nohighlight">
\[\nabla_{\theta} J(\pi_{\theta}) &amp;= \sum_{t=0}^{T} \underE{\tau_{:t} \sim \pi_{\theta}}{ \underE{\tau_{t:} \sim \pi_{\theta}}{ \left. \nabla_{\theta} \log \pi_{\theta}(a_t |s_t) \hat{R}_t \right| \tau_{:t}}}\]</div>
<p>The grad-log-prob is constant with respect to the inner expectation (because it depends on <span class="math notranslate nohighlight">\(s_t\)</span> and <span class="math notranslate nohighlight">\(a_t\)</span>, which the inner expectation conditions on as fixed in <span class="math notranslate nohighlight">\(\tau_{:t}\)</span>), so it can be pulled out, leaving:</p>
<div class="math notranslate nohighlight">
\[\nabla_{\theta} J(\pi_{\theta}) &amp;= \sum_{t=0}^{T} \underE{\tau_{:t} \sim \pi_{\theta}}{ \nabla_{\theta} \log \pi_{\theta}(a_t |s_t) \underE{\tau_{t:} \sim \pi_{\theta}}{ \left. \hat{R}_t \right| \tau_{:t}}}\]</div>
<p>In Markov Decision Processes, the future only depends on the most recent state and action. As a result, the inner expectation—which expects over the future, conditioned on the entirety of the past (everything up to time <span class="math notranslate nohighlight">\(t\)</span>)—is equal to the same expectation if it only conditioned on the last timestep (just <span class="math notranslate nohighlight">\((s_t,a_t)\)</span>):</p>
<div class="math notranslate nohighlight">
\[\underE{\tau_{t:} \sim \pi_{\theta}}{ \left. \hat{R}_t \right| \tau_{:t}} = \underE{\tau_{t:} \sim \pi_{\theta}}{ \left. \hat{R}_t \right| s_t, a_t},\]</div>
<p>which is the <em>definition</em> of <span class="math notranslate nohighlight">\(Q^{\pi_{\theta}}(s_t, a_t)\)</span>: the expected return, starting from state <span class="math notranslate nohighlight">\(s_t\)</span> and action <span class="math notranslate nohighlight">\(a_t\)</span>, when acting on-policy for the rest of the trajectory.</p>
<p>The result follows immediately.</p>
</section>
</section>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../../../../index.html">multi-agent-market-rl</a></h1>








<h3>Navigation</h3>
<p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../MultiAgentMarketRL.html">Multi-Agent-Market-RL</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../../../../index.html">Documentation overview</a><ul>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../../../../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>$('#searchbox').show(0);</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2021, Benjamin Suter.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 4.1.1</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="../../../../_sources/submodules/spinningup-rl-tutorial/docs/spinningup/extra_pg_proof2.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>