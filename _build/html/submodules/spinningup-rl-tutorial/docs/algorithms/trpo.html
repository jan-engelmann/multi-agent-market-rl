
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Trust Region Policy Optimization &#8212; multi-agent-market-rl 26.7.21 documentation</title>
    <link rel="stylesheet" type="text/css" href="../../../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/alabaster.css" />
    <script data-url_root="../../../../" id="documentation_options" src="../../../../_static/documentation_options.js"></script>
    <script src="../../../../_static/jquery.js"></script>
    <script src="../../../../_static/underscore.js"></script>
    <script src="../../../../_static/doctools.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" />
   
  <link rel="stylesheet" href="../../../../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <section id="trust-region-policy-optimization">
<h1><a class="toc-backref" href="#id4">Trust Region Policy Optimization</a><a class="headerlink" href="#trust-region-policy-optimization" title="Permalink to this headline">¶</a></h1>
<div class="contents topic" id="table-of-contents">
<p class="topic-title">Table of Contents</p>
<ul class="simple">
<li><p><a class="reference internal" href="#trust-region-policy-optimization" id="id4">Trust Region Policy Optimization</a></p>
<ul>
<li><p><a class="reference internal" href="#background" id="id5">Background</a></p>
<ul>
<li><p><a class="reference internal" href="#quick-facts" id="id6">Quick Facts</a></p></li>
<li><p><a class="reference internal" href="#key-equations" id="id7">Key Equations</a></p></li>
<li><p><a class="reference internal" href="#exploration-vs-exploitation" id="id8">Exploration vs. Exploitation</a></p></li>
<li><p><a class="reference internal" href="#pseudocode" id="id9">Pseudocode</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#documentation" id="id10">Documentation</a></p>
<ul>
<li><p><a class="reference internal" href="#saved-model-contents" id="id11">Saved Model Contents</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#references" id="id12">References</a></p>
<ul>
<li><p><a class="reference internal" href="#relevant-papers" id="id13">Relevant Papers</a></p></li>
<li><p><a class="reference internal" href="#why-these-papers" id="id14">Why These Papers?</a></p></li>
<li><p><a class="reference internal" href="#other-public-implementations" id="id15">Other Public Implementations</a></p></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
<section id="background">
<h2><a class="toc-backref" href="#id5">Background</a><a class="headerlink" href="#background" title="Permalink to this headline">¶</a></h2>
<p>(Previously: <a class="reference external" href="../algorithms/vpg.html#background">Background for VPG</a>)</p>
<p>TRPO updates policies by taking the largest step possible to improve performance, while satisfying a special constraint on how close the new and old policies are allowed to be. The constraint is expressed in terms of <a class="reference external" href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence">KL-Divergence</a>, a measure of (something like, but not exactly) distance between probability distributions.</p>
<p>This is different from normal policy gradient, which keeps new and old policies close in parameter space. But even seemingly small differences in parameter space can have very large differences in performance—so a single bad step can collapse the policy performance. This makes it dangerous to use large step sizes with vanilla policy gradients, thus hurting its sample efficiency. TRPO nicely avoids this kind of collapse, and tends to quickly and monotonically improve performance.</p>
<section id="quick-facts">
<h3><a class="toc-backref" href="#id6">Quick Facts</a><a class="headerlink" href="#quick-facts" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>TRPO is an on-policy algorithm.</p></li>
<li><p>TRPO can be used for environments with either discrete or continuous action spaces.</p></li>
<li><p>The Spinning Up implementation of TRPO supports parallelization with MPI.</p></li>
</ul>
</section>
<section id="key-equations">
<h3><a class="toc-backref" href="#id7">Key Equations</a><a class="headerlink" href="#key-equations" title="Permalink to this headline">¶</a></h3>
<p>Let <span class="math notranslate nohighlight">\(\pi_{\theta}\)</span> denote a policy with parameters <span class="math notranslate nohighlight">\(\theta\)</span>. The theoretical TRPO update is:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\theta_{k+1} = \arg \max_{\theta} \; &amp; {\mathcal L}(\theta_k, \theta) \\
\text{s.t.} \; &amp; \bar{D}_{KL}(\theta || \theta_k) \leq \delta\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\({\mathcal L}(\theta_k, \theta)\)</span> is the <em>surrogate advantage</em>, a measure of how policy <span class="math notranslate nohighlight">\(\pi_{\theta}\)</span> performs relative to the old policy <span class="math notranslate nohighlight">\(\pi_{\theta_k}\)</span> using data from the old policy:</p>
<div class="math notranslate nohighlight">
\[{\mathcal L}(\theta_k, \theta) = \underE{s,a \sim \pi_{\theta_k}}{
    \frac{\pi_{\theta}(a|s)}{\pi_{\theta_k}(a|s)} A^{\pi_{\theta_k}}(s,a)
    },\]</div>
<p>and <span class="math notranslate nohighlight">\(\bar{D}_{KL}(\theta || \theta_k)\)</span> is an average KL-divergence between policies across states visited by the old policy:</p>
<div class="math notranslate nohighlight">
\[\bar{D}_{KL}(\theta || \theta_k) = \underE{s \sim \pi_{\theta_k}}{
    D_{KL}\left(\pi_{\theta}(\cdot|s) || \pi_{\theta_k} (\cdot|s) \right)
}.\]</div>
<div class="admonition-you-should-know admonition">
<p class="admonition-title">You Should Know</p>
<p>The objective and constraint are both zero when <span class="math notranslate nohighlight">\(\theta = \theta_k\)</span>. Furthermore, the gradient of the constraint with respect to <span class="math notranslate nohighlight">\(\theta\)</span> is zero when <span class="math notranslate nohighlight">\(\theta = \theta_k\)</span>. Proving these facts requires some subtle command of the relevant math—it’s an exercise worth doing, whenever you feel ready!</p>
</div>
<p>The theoretical TRPO update isn’t the easiest to work with, so TRPO makes some approximations to get an answer quickly. We Taylor expand the objective and constraint to leading order around <span class="math notranslate nohighlight">\(\theta_k\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}{\mathcal L}(\theta_k, \theta) &amp;\approx g^T (\theta - \theta_k) \\
\bar{D}_{KL}(\theta || \theta_k) &amp; \approx \frac{1}{2} (\theta - \theta_k)^T H (\theta - \theta_k)\end{split}\]</div>
<p>resulting in an approximate optimization problem,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\theta_{k+1} = \arg \max_{\theta} \; &amp; g^T (\theta - \theta_k) \\
\text{s.t.} \; &amp; \frac{1}{2} (\theta - \theta_k)^T H (\theta - \theta_k) \leq \delta.\end{split}\]</div>
<div class="admonition-you-should-know admonition">
<p class="admonition-title">You Should Know</p>
<p>By happy coincidence, the gradient <span class="math notranslate nohighlight">\(g\)</span> of the surrogate advantage function with respect to <span class="math notranslate nohighlight">\(\theta\)</span>, evaluated at <span class="math notranslate nohighlight">\(\theta = \theta_k\)</span>, is exactly equal to the policy gradient, <span class="math notranslate nohighlight">\(\nabla_{\theta} J(\pi_{\theta})\)</span>! Try proving this, if you feel comfortable diving into the math.</p>
</div>
<p>This approximate problem can be analytically solved by the methods of Lagrangian duality <a class="footnote-reference brackets" href="#id2" id="id1">1</a>, yielding the solution:</p>
<div class="math notranslate nohighlight">
\[\theta_{k+1} = \theta_k + \sqrt{\frac{2 \delta}{g^T H^{-1} g}} H^{-1} g.\]</div>
<p>If we were to stop here, and just use this final result, the algorithm would be exactly calculating the <a class="reference external" href="https://papers.nips.cc/paper/2073-a-natural-policy-gradient.pdf">Natural Policy Gradient</a>. A problem is that, due to the approximation errors introduced by the Taylor expansion, this may not satisfy the KL constraint, or actually improve the surrogate advantage. TRPO adds a modification to this update rule: a backtracking line search,</p>
<div class="math notranslate nohighlight">
\[\theta_{k+1} = \theta_k + \alpha^j \sqrt{\frac{2 \delta}{g^T H^{-1} g}} H^{-1} g,\]</div>
<p>where <span class="math notranslate nohighlight">\(\alpha \in (0,1)\)</span> is the backtracking coefficient, and <span class="math notranslate nohighlight">\(j\)</span> is the smallest nonnegative integer such that <span class="math notranslate nohighlight">\(\pi_{\theta_{k+1}}\)</span> satisfies the KL constraint and produces a positive surrogate advantage.</p>
<p>Lastly: computing and storing the matrix inverse, <span class="math notranslate nohighlight">\(H^{-1}\)</span>, is painfully expensive when dealing with neural network policies with thousands or millions of parameters. TRPO sidesteps the issue by using the <a class="reference external" href="https://en.wikipedia.org/wiki/Conjugate_gradient_method">conjugate gradient</a> algorithm to solve <span class="math notranslate nohighlight">\(Hx = g\)</span> for <span class="math notranslate nohighlight">\(x = H^{-1} g\)</span>, requiring only a function which can compute the matrix-vector product <span class="math notranslate nohighlight">\(Hx\)</span> instead of computing and storing the whole matrix <span class="math notranslate nohighlight">\(H\)</span> directly. This is not too hard to do: we set up a symbolic operation to calculate</p>
<div class="math notranslate nohighlight">
\[Hx = \nabla_{\theta} \left( \left(\nabla_{\theta} \bar{D}_{KL}(\theta || \theta_k)\right)^T x \right),\]</div>
<p>which gives us the correct output without computing the whole matrix.</p>
<dl class="footnote brackets">
<dt class="label" id="id2"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd><p>See <a class="reference external" href="http://stanford.edu/~boyd/cvxbook/">Convex Optimization</a> by Boyd and Vandenberghe, especially chapters 2 through 5.</p>
</dd>
</dl>
</section>
<section id="exploration-vs-exploitation">
<h3><a class="toc-backref" href="#id8">Exploration vs. Exploitation</a><a class="headerlink" href="#exploration-vs-exploitation" title="Permalink to this headline">¶</a></h3>
<p>TRPO trains a stochastic policy in an on-policy way. This means that it explores by sampling actions according to the latest version of its stochastic policy. The amount of randomness in action selection depends on both initial conditions and the training procedure. Over the course of training, the policy typically becomes progressively less random, as the update rule encourages it to exploit rewards that it has already found. This may cause the policy to get trapped in local optima.</p>
</section>
<section id="pseudocode">
<h3><a class="toc-backref" href="#id9">Pseudocode</a><a class="headerlink" href="#pseudocode" title="Permalink to this headline">¶</a></h3>
<div class="math notranslate nohighlight">
\begin{algorithm}[H]
    \caption{Trust Region Policy Optimization}
    \label{alg1}
\begin{algorithmic}[1]
    \STATE Input: initial policy parameters $\theta_0$, initial value function parameters $\phi_0$
    \STATE Hyperparameters: KL-divergence limit $\delta$, backtracking coefficient $\alpha$, maximum number of backtracking steps $K$
    \FOR{$k = 0,1,2,...$}
    \STATE Collect set of trajectories ${\mathcal D}_k = \{\tau_i\}$ by running policy $\pi_k = \pi(\theta_k)$ in the environment.
    \STATE Compute rewards-to-go $\hat{R}_t$.
    \STATE Compute advantage estimates, $\hat{A}_t$ (using any method of advantage estimation) based on the current value function $V_{\phi_k}$.
    \STATE Estimate policy gradient as
        \begin{equation*}
        \hat{g}_k = \frac{1}{|{\mathcal D}_k|} \sum_{\tau \in {\mathcal D}_k} \sum_{t=0}^T \left. \nabla_{\theta} \log\pi_{\theta}(a_t|s_t)\right|_{\theta_k} \hat{A}_t.
        \end{equation*}
    \STATE Use the conjugate gradient algorithm to compute
        \begin{equation*}
        \hat{x}_k \approx \hat{H}_k^{-1} \hat{g}_k,
        \end{equation*}
        where $\hat{H}_k$ is the Hessian of the sample average KL-divergence.
    \STATE Update the policy by backtracking line search with
        \begin{equation*}
        \theta_{k+1} = \theta_k + \alpha^j \sqrt{ \frac{2\delta}{\hat{x}_k^T \hat{H}_k \hat{x}_k}} \hat{x}_k,
        \end{equation*}
        where $j \in \{0, 1, 2, ... K\}$ is the smallest value which improves the sample loss and satisfies the sample KL-divergence constraint.
    \STATE Fit value function by regression on mean-squared error:
        \begin{equation*}
        \phi_{k+1} = \arg \min_{\phi} \frac{1}{|{\mathcal D}_k| T} \sum_{\tau \in {\mathcal D}_k} \sum_{t=0}^T\left( V_{\phi} (s_t) - \hat{R}_t \right)^2,
        \end{equation*}
        typically via some gradient descent algorithm.
    \ENDFOR
\end{algorithmic}
\end{algorithm}</div></section>
</section>
<section id="documentation">
<h2><a class="toc-backref" href="#id10">Documentation</a><a class="headerlink" href="#documentation" title="Permalink to this headline">¶</a></h2>
<div class="admonition-you-should-know admonition">
<p class="admonition-title">You Should Know</p>
<p>Spinning Up currently only has a Tensorflow implementation of TRPO.</p>
</div>
<section id="saved-model-contents">
<h3><a class="toc-backref" href="#id11">Saved Model Contents</a><a class="headerlink" href="#saved-model-contents" title="Permalink to this headline">¶</a></h3>
<p>The computation graph saved by the logger includes:</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 11%" />
<col style="width: 89%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Key</p></th>
<th class="head"><p>Value</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">x</span></code></p></td>
<td><p>Tensorflow placeholder for state input.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">pi</span></code></p></td>
<td><p>Samples an action from the agent, conditioned on states in <code class="docutils literal notranslate"><span class="pre">x</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">v</span></code></p></td>
<td><p>Gives value estimate for states in <code class="docutils literal notranslate"><span class="pre">x</span></code>.</p></td>
</tr>
</tbody>
</table>
<p>This saved model can be accessed either by</p>
<ul class="simple">
<li><p>running the trained policy with the <a class="reference external" href="../user/saving_and_loading.html#loading-and-running-trained-policies">test_policy.py</a> tool,</p></li>
<li><p>or loading the whole saved graph into a program with <a class="reference external" href="../utils/logger.html#spinup.utils.logx.restore_tf_graph">restore_tf_graph</a>.</p></li>
</ul>
</section>
</section>
<section id="references">
<h2><a class="toc-backref" href="#id12">References</a><a class="headerlink" href="#references" title="Permalink to this headline">¶</a></h2>
<section id="relevant-papers">
<h3><a class="toc-backref" href="#id13">Relevant Papers</a><a class="headerlink" href="#relevant-papers" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p><a class="reference external" href="https://arxiv.org/abs/1502.05477">Trust Region Policy Optimization</a>, Schulman et al. 2015</p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/1506.02438">High Dimensional Continuous Control Using Generalized Advantage Estimation</a>, Schulman et al. 2016</p></li>
<li><p><a class="reference external" href="https://people.eecs.berkeley.edu/~pabbeel/cs287-fa09/readings/KakadeLangford-icml2002.pdf">Approximately Optimal Approximate Reinforcement Learning</a>, Kakade and Langford 2002</p></li>
</ul>
</section>
<section id="why-these-papers">
<h3><a class="toc-backref" href="#id14">Why These Papers?</a><a class="headerlink" href="#why-these-papers" title="Permalink to this headline">¶</a></h3>
<p>Schulman 2015 is included because it is the original paper describing TRPO. Schulman 2016 is included because our implementation of TRPO makes use of Generalized Advantage Estimation for computing the policy gradient. Kakade and Langford 2002 is included because it contains theoretical results which motivate and deeply connect to the theoretical foundations of TRPO.</p>
</section>
<section id="other-public-implementations">
<h3><a class="toc-backref" href="#id15">Other Public Implementations</a><a class="headerlink" href="#other-public-implementations" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/openai/baselines/tree/master/baselines/trpo_mpi">Baselines</a></p></li>
<li><p><a class="reference external" href="https://github.com/joschu/modular_rl/blob/master/modular_rl/trpo.py">ModularRL</a></p></li>
<li><p><a class="reference external" href="https://github.com/rll/rllab/blob/master/rllab/algos/trpo.py">rllab</a></p></li>
</ul>
</section>
</section>
</section>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../../../../index.html">multi-agent-market-rl</a></h1>








<h3>Navigation</h3>
<p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../MultiAgentMarketRL.html">Multi-Agent-Market-RL</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../../../../index.html">Documentation overview</a><ul>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../../../../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>$('#searchbox').show(0);</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2021, Benjamin Suter.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 4.1.1</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="../../../../_sources/submodules/spinningup-rl-tutorial/docs/algorithms/trpo.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>