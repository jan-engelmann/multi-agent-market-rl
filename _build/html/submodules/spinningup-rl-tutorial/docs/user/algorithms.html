
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Algorithms &#8212; multi-agent-market-rl 26.7.21 documentation</title>
    <link rel="stylesheet" type="text/css" href="../../../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/alabaster.css" />
    <script data-url_root="../../../../" id="documentation_options" src="../../../../_static/documentation_options.js"></script>
    <script src="../../../../_static/jquery.js"></script>
    <script src="../../../../_static/underscore.js"></script>
    <script src="../../../../_static/doctools.js"></script>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" />
   
  <link rel="stylesheet" href="../../../../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <section id="algorithms">
<h1><a class="toc-backref" href="#id1">Algorithms</a><a class="headerlink" href="#algorithms" title="Permalink to this headline">¶</a></h1>
<div class="contents topic" id="table-of-contents">
<p class="topic-title">Table of Contents</p>
<ul class="simple">
<li><p><a class="reference internal" href="#algorithms" id="id1">Algorithms</a></p>
<ul>
<li><p><a class="reference internal" href="#what-s-included" id="id2">What’s Included</a></p></li>
<li><p><a class="reference internal" href="#why-these-algorithms" id="id3">Why These Algorithms?</a></p>
<ul>
<li><p><a class="reference internal" href="#the-on-policy-algorithms" id="id4">The On-Policy Algorithms</a></p></li>
<li><p><a class="reference internal" href="#the-off-policy-algorithms" id="id5">The Off-Policy Algorithms</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#code-format" id="id6">Code Format</a></p>
<ul>
<li><p><a class="reference internal" href="#the-algorithm-function-pytorch-version" id="id7">The Algorithm Function: PyTorch Version</a></p></li>
<li><p><a class="reference internal" href="#the-algorithm-function-tensorflow-version" id="id8">The Algorithm Function: Tensorflow Version</a></p></li>
<li><p><a class="reference internal" href="#the-core-file" id="id9">The Core File</a></p></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
<section id="what-s-included">
<h2><a class="toc-backref" href="#id2">What’s Included</a><a class="headerlink" href="#what-s-included" title="Permalink to this headline">¶</a></h2>
<p>The following algorithms are implemented in the Spinning Up package:</p>
<ul class="simple">
<li><p><a class="reference external" href="../algorithms/vpg.html">Vanilla Policy Gradient</a> (VPG)</p></li>
<li><p><a class="reference external" href="../algorithms/trpo.html">Trust Region Policy Optimization</a> (TRPO)</p></li>
<li><p><a class="reference external" href="../algorithms/ppo.html">Proximal Policy Optimization</a> (PPO)</p></li>
<li><p><a class="reference external" href="../algorithms/ddpg.html">Deep Deterministic Policy Gradient</a> (DDPG)</p></li>
<li><p><a class="reference external" href="../algorithms/td3.html">Twin Delayed DDPG</a> (TD3)</p></li>
<li><p><a class="reference external" href="../algorithms/sac.html">Soft Actor-Critic</a> (SAC)</p></li>
</ul>
<p>They are all implemented with <a class="reference external" href="https://en.wikipedia.org/wiki/Multilayer_perceptron">MLP</a> (non-recurrent) actor-critics, making them suitable for fully-observed, non-image-based RL environments, e.g. the <a class="reference external" href="https://gym.openai.com/envs/#mujoco">Gym Mujoco</a> environments.</p>
<p>Spinning Up has two implementations for each algorithm (except for TRPO): one that uses <a class="reference external" href="https://pytorch.org/">PyTorch</a> as the neural network library, and one that uses <a class="reference external" href="https://www.tensorflow.org/versions/r1.15/api_docs">Tensorflow v1</a> as the neural network library. (TRPO is currently only available in Tensorflow.)</p>
</section>
<section id="why-these-algorithms">
<h2><a class="toc-backref" href="#id3">Why These Algorithms?</a><a class="headerlink" href="#why-these-algorithms" title="Permalink to this headline">¶</a></h2>
<p>We chose the core deep RL algorithms in this package to reflect useful progressions of ideas from the recent history of the field, culminating in two algorithms in particular—PPO and SAC—which are close to state of the art on reliability and sample efficiency among policy-learning algorithms. They also expose some of the trade-offs that get made in designing and using algorithms in deep RL.</p>
<section id="the-on-policy-algorithms">
<h3><a class="toc-backref" href="#id4">The On-Policy Algorithms</a><a class="headerlink" href="#the-on-policy-algorithms" title="Permalink to this headline">¶</a></h3>
<p>Vanilla Policy Gradient is the most basic, entry-level algorithm in the deep RL space because it completely predates the advent of deep RL altogether. The core elements of VPG go all the way back to the late 80s / early 90s. It started a trail of research which ultimately led to stronger algorithms such as TRPO and then PPO soon after.</p>
<p>A key feature of this line of work is that all of these algorithms are <em>on-policy</em>: that is, they don’t use old data, which makes them weaker on sample efficiency. But this is for a good reason: these algorithms directly optimize the objective you care about—policy performance—and it works out mathematically that you need on-policy data to calculate the updates. So, this family of algorithms trades off sample efficiency in favor of stability—but you can see the progression of techniques (from VPG to TRPO to PPO) working to make up the deficit on sample efficiency.</p>
</section>
<section id="the-off-policy-algorithms">
<h3><a class="toc-backref" href="#id5">The Off-Policy Algorithms</a><a class="headerlink" href="#the-off-policy-algorithms" title="Permalink to this headline">¶</a></h3>
<p>DDPG is a similarly foundational algorithm to VPG, although much younger—the theory of deterministic policy gradients, which led to DDPG, wasn’t published until 2014. DDPG is closely connected to Q-learning algorithms, and it concurrently learns a Q-function and a policy which are updated to improve each other.</p>
<p>Algorithms like DDPG and Q-Learning are <em>off-policy</em>, so they are able to reuse old data very efficiently. They gain this benefit by exploiting Bellman’s equations for optimality, which a Q-function can be trained to satisfy using <em>any</em> environment interaction data (as long as there’s enough experience from the high-reward areas in the environment).</p>
<p>But problematically, there are no guarantees that doing a good job of satisfying Bellman’s equations leads to having great policy performance. <em>Empirically</em> one can get great performance—and when it happens, the sample efficiency is wonderful—but the absence of guarantees makes algorithms in this class potentially brittle and unstable. TD3 and SAC are descendants of DDPG which make use of a variety of insights to mitigate these issues.</p>
</section>
</section>
<section id="code-format">
<h2><a class="toc-backref" href="#id6">Code Format</a><a class="headerlink" href="#code-format" title="Permalink to this headline">¶</a></h2>
<p>All implementations in Spinning Up adhere to a standard template. They are split into two files: an algorithm file, which contains the core logic of the algorithm, and a core file, which contains various utilities needed to run the algorithm.</p>
<p>The algorithm file always starts with a class definition for an experience buffer object, which is used to store information from agent-environment interactions. Next, there is a single function which runs the algorithm. The algorithm function follows a template that is roughly the same across the PyTorch and Tensorflow versions, but we’ll break it down for each separately below. Finally, there’s some support in each algorithm file for directly running the algorithm in Gym environments from the command line (though this is not the recommended way to run the algorithms—we’ll describe how to do that on the <a class="reference external" href="../user/running.html">Running Experiments</a> page).</p>
<section id="the-algorithm-function-pytorch-version">
<h3><a class="toc-backref" href="#id7">The Algorithm Function: PyTorch Version</a><a class="headerlink" href="#the-algorithm-function-pytorch-version" title="Permalink to this headline">¶</a></h3>
<p>The algorithm function for a PyTorch implementation performs the following tasks in (roughly) this order:</p>
<blockquote>
<div><ol class="arabic simple">
<li><p>Logger setup</p></li>
<li><p>Random seed setting</p></li>
<li><p>Environment instantiation</p></li>
<li><p>Constructing the actor-critic PyTorch module via the <code class="docutils literal notranslate"><span class="pre">actor_critic</span></code> function passed to the algorithm function as an argument</p></li>
<li><p>Instantiating the experience buffer</p></li>
<li><p>Setting up callable loss functions that also provide diagnostics specific to the algorithm</p></li>
<li><p>Making PyTorch optimizers</p></li>
<li><p>Setting up model saving through the logger</p></li>
<li><p>Setting up an update function that runs one epoch of optimization or one step of descent</p></li>
<li><p>Running the main loop of the algorithm:</p>
<ol class="loweralpha simple">
<li><p>Run the agent in the environment</p></li>
<li><p>Periodically update the parameters of the agent according to the main equations of the algorithm</p></li>
<li><p>Log key performance metrics and save agent</p></li>
</ol>
</li>
</ol>
</div></blockquote>
</section>
<section id="the-algorithm-function-tensorflow-version">
<h3><a class="toc-backref" href="#id8">The Algorithm Function: Tensorflow Version</a><a class="headerlink" href="#the-algorithm-function-tensorflow-version" title="Permalink to this headline">¶</a></h3>
<p>The algorithm function for a Tensorflow implementation performs the following tasks in (roughly) this order:</p>
<blockquote>
<div><ol class="arabic simple">
<li><p>Logger setup</p></li>
<li><p>Random seed setting</p></li>
<li><p>Environment instantiation</p></li>
<li><p>Making placeholders for the computation graph</p></li>
<li><p>Building the actor-critic computation graph via the <code class="docutils literal notranslate"><span class="pre">actor_critic</span></code> function passed to the algorithm function as an argument</p></li>
<li><p>Instantiating the experience buffer</p></li>
<li><p>Building the computation graph for loss functions and diagnostics specific to the algorithm</p></li>
<li><p>Making training ops</p></li>
<li><p>Making the TF Session and initializing parameters</p></li>
<li><p>Setting up model saving through the logger</p></li>
<li><p>Defining functions needed for running the main loop of the algorithm (e.g. the core update function, get action function, and test agent function, depending on the algorithm)</p></li>
<li><p>Running the main loop of the algorithm:</p>
<ol class="loweralpha simple">
<li><p>Run the agent in the environment</p></li>
<li><p>Periodically update the parameters of the agent according to the main equations of the algorithm</p></li>
<li><p>Log key performance metrics and save agent</p></li>
</ol>
</li>
</ol>
</div></blockquote>
</section>
<section id="the-core-file">
<h3><a class="toc-backref" href="#id9">The Core File</a><a class="headerlink" href="#the-core-file" title="Permalink to this headline">¶</a></h3>
<p>The core files don’t adhere as closely as the algorithms files to a template, but do have some approximate structure:</p>
<blockquote>
<div><ol class="arabic simple">
<li><p><strong>Tensorflow only:</strong> Functions related to making and managing placeholders</p></li>
<li><p>Functions for building sections of computation graph relevant to the <code class="docutils literal notranslate"><span class="pre">actor_critic</span></code> method for a particular algorithm</p></li>
<li><p>Any other useful functions</p></li>
<li><p>Implementations for an MLP actor-critic compatible with the algorithm, where both the policy and the value function(s) are represented by simple MLPs</p></li>
</ol>
</div></blockquote>
</section>
</section>
</section>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../../../../index.html">multi-agent-market-rl</a></h1>








<h3>Navigation</h3>
<p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../MultiAgentMarketRL.html">Multi-Agent-Market-RL</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../../../../index.html">Documentation overview</a><ul>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../../../../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>$('#searchbox').show(0);</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2021, Benjamin Suter.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 4.1.1</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="../../../../_sources/submodules/spinningup-rl-tutorial/docs/user/algorithms.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>