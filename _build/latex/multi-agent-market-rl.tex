%% Generated by Sphinx.
\def\sphinxdocclass{report}
\documentclass[letterpaper,10pt,english]{sphinxmanual}
\ifdefined\pdfpxdimen
   \let\sphinxpxdimen\pdfpxdimen\else\newdimen\sphinxpxdimen
\fi \sphinxpxdimen=.75bp\relax
\ifdefined\pdfimageresolution
    \pdfimageresolution= \numexpr \dimexpr1in\relax/\sphinxpxdimen\relax
\fi
%% let collapsable pdf bookmarks panel have high depth per default
\PassOptionsToPackage{bookmarksdepth=5}{hyperref}

\PassOptionsToPackage{warn}{textcomp}
\usepackage[utf8]{inputenc}
\ifdefined\DeclareUnicodeCharacter
% support both utf8 and utf8x syntaxes
  \ifdefined\DeclareUnicodeCharacterAsOptional
    \def\sphinxDUC#1{\DeclareUnicodeCharacter{"#1}}
  \else
    \let\sphinxDUC\DeclareUnicodeCharacter
  \fi
  \sphinxDUC{00A0}{\nobreakspace}
  \sphinxDUC{2500}{\sphinxunichar{2500}}
  \sphinxDUC{2502}{\sphinxunichar{2502}}
  \sphinxDUC{2514}{\sphinxunichar{2514}}
  \sphinxDUC{251C}{\sphinxunichar{251C}}
  \sphinxDUC{2572}{\textbackslash}
\fi
\usepackage{cmap}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amstext}
\usepackage{babel}



\usepackage{tgtermes}
\usepackage{tgheros}
\renewcommand{\ttdefault}{txtt}



\usepackage[Bjarne]{fncychap}
\usepackage{sphinx}

\fvset{fontsize=auto}
\usepackage{geometry}


% Include hyperref last.
\usepackage{hyperref}
% Fix anchor placement for figures with captions.
\usepackage{hypcap}% it must be loaded after hyperref.
% Set up styles of URL: it should be placed after hyperref.
\urlstyle{same}

\addto\captionsenglish{\renewcommand{\contentsname}{Contents:}}

\usepackage{sphinxmessages}
\setcounter{tocdepth}{1}



\title{multi\sphinxhyphen{}agent\sphinxhyphen{}market\sphinxhyphen{}rl}
\date{Aug 22, 2021}
\release{0.0.1}
\author{Benjamin Suter}
\newcommand{\sphinxlogo}{\vbox{}}
\renewcommand{\releasename}{Release}
\makeindex
\begin{document}

\pagestyle{empty}
\sphinxmaketitle
\pagestyle{plain}
\sphinxtableofcontents
\pagestyle{normal}
\phantomsection\label{\detokenize{index::doc}}



\chapter{Installation}
\label{\detokenize{installation:installation}}\label{\detokenize{installation::doc}}
\sphinxAtStartPar
For a local installation, follow the below instructions.
We highly recommend installing the following application into a virtual environment.
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Clone this repository.:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{git} \PYG{n}{clone} \PYG{n}{https}\PYG{p}{:}\PYG{o}{/}\PYG{o}{/}\PYG{n}{github}\PYG{o}{.}\PYG{n}{com}\PYG{o}{/}\PYG{n}{jan}\PYG{o}{\PYGZhy{}}\PYG{n}{engelmann}\PYG{o}{/}\PYG{n}{multi}\PYG{o}{\PYGZhy{}}\PYG{n}{agent}\PYG{o}{\PYGZhy{}}\PYG{n}{market}\PYG{o}{\PYGZhy{}}\PYG{n}{rl}
\PYG{n}{cd} \PYG{n}{multi}\PYG{o}{\PYGZhy{}}\PYG{n}{agent}\PYG{o}{\PYGZhy{}}\PYG{n}{market}\PYG{o}{\PYGZhy{}}\PYG{n}{rl}
\end{sphinxVerbatim}

\item {} 
\sphinxAtStartPar
Install the dependencies:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{pip} \PYG{n}{install} \PYG{o}{\PYGZhy{}}\PYG{n}{r} \PYG{n}{requirements}\PYG{o}{.}\PYG{n}{txt}
\end{sphinxVerbatim}

\item {} 
\sphinxAtStartPar
Install the package:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{pip} \PYG{n}{install} \PYG{o}{\PYGZhy{}}\PYG{n}{e} \PYG{o}{.}
\end{sphinxVerbatim}

\end{enumerate}


\chapter{Quick guide to using the environment}
\label{\detokenize{UsingTheEnvironment:quick-guide-to-using-the-environment}}\label{\detokenize{UsingTheEnvironment::doc}}

\section{The agent dictionary}
\label{\detokenize{UsingTheEnvironment:the-agent-dictionary}}\label{\detokenize{UsingTheEnvironment:agent-dict}}
\sphinxAtStartPar
The agent dictionary is used to inform the environment of how many agents and of what type and with what specifications
will be participating. The agent dictionary has a nested dictionary structure. More specifically, every agent with the
role \sphinxstyleemphasis{seller} and every agent with the role \sphinxstyleemphasis{buyer} are collected in separate dictionaries.

\sphinxAtStartPar
\sphinxcode{\sphinxupquote{agent\_dict = \{\textquotesingle{}sellers\textquotesingle{}: seller\_agent\_dict, \textquotesingle{}buyers\textquotesingle{}: buyer\_agent\_dict\}}}

\sphinxAtStartPar
The \sphinxstylestrong{seller\_agent\_dict} is again a nested dictionary made up of \sphinxstyleemphasis{n} (number of selling agent configurations) single
agent dictionaries (same holds for the \sphinxstylestrong{buyer\_agent\_dict}).

\sphinxAtStartPar
\sphinxcode{\sphinxupquote{seller\_agent\_dict = \{1: single\_agent\_seller\_1, ..., n: single\_agent\_seller\_n\}
buyer\_agent\_dict = \{1: single\_agent\_buyer\_1, ..., m: single\_agent\_buyer\_m\}}}

\sphinxAtStartPar
Finally a single agent dictionary is made up of the following \sphinxcode{\sphinxupquote{key:value}} pairs:
\begin{description}
\item[{\sphinxstylestrong{Mandatory key:value pairs}}] \leavevmode\begin{itemize}
\item {} \begin{description}
\item[{‘type’ (str)}] \leavevmode
\sphinxAtStartPar
The name of the wanted agent class object

\end{description}

\item {} \begin{description}
\item[{‘reservation’ (int)}] \leavevmode
\sphinxAtStartPar
The reservation price for this agent

\end{description}

\end{itemize}

\item[{\sphinxstylestrong{Optional key:value pairs}}] \leavevmode\begin{itemize}
\item {} \begin{description}
\item[{‘multiplicity’ (int)}] \leavevmode
\sphinxAtStartPar
The multiplicity count of the specific agent implementation (default=1)

\end{description}

\item {} \begin{description}
\item[{{\color{red}\bfseries{}**}kwargs}] \leavevmode
\sphinxAtStartPar
Additional keyword arguments specific to the chosen agent type

\end{description}

\end{itemize}

\end{description}

\sphinxAtStartPar
\sphinxcode{\sphinxupquote{single\_agent\_dict = \{\textquotesingle{}type\textquotesingle{}: \textquotesingle{}MyAgentType\textquotesingle{}, \textquotesingle{}reservation\textquotesingle{}: 12, \textquotesingle{}multiplicity\textquotesingle{}: 3, **kwargs\}}}

\sphinxAtStartPar
The agent types currently implemented have the following type specific \sphinxcode{\sphinxupquote{kwargs}}:
\begin{description}
\item[{\sphinxstylestrong{DQNAgent}}] \leavevmode\begin{itemize}
\item {} \begin{description}
\item[{network\_type: str, optional (default=”SimpleExampleNetwork”)}] \leavevmode
\sphinxAtStartPar
Name of network class implemented in network\_models.py

\end{description}

\item {} \begin{description}
\item[{q\_lr: float (default=0.001)}] \leavevmode
\sphinxAtStartPar
Learning rate provided to the Q\sphinxhyphen{}Network optimizer

\end{description}

\item {} \begin{description}
\item[{save\_weights\_directory: str (default=”../saved\_agent\_weights/default\_path/\{self.agent\_name\}/”)}] \leavevmode
\sphinxAtStartPar
Directory to where model weights will be saved to

\end{description}

\item {} \begin{description}
\item[{save\_weights\_file: str (default=”default\_test\_file.pt”)}] \leavevmode
\sphinxAtStartPar
File name of the saved weights. Must be a .pt or .pth file

\end{description}

\item {} \begin{description}
\item[{load\_weights\_path: str (default=False)}] \leavevmode
\sphinxAtStartPar
If a path is provided, agent will try to load pretrained weights from there

\end{description}

\end{itemize}

\item[{\sphinxstylestrong{ConstAgent}}] \leavevmode\begin{itemize}
\item {} \begin{description}
\item[{const\_price: int (default=Mean value of action space)}] \leavevmode
\sphinxAtStartPar
The constant asking / bidding price

\end{description}

\end{itemize}

\item[{\sphinxstylestrong{HumanReplayAgent}}] \leavevmode\begin{itemize}
\item {} \begin{description}
\item[{data\_type: str (default=’new\_data’)}] \leavevmode
\sphinxAtStartPar
Data set used (new\_data or old\_data). See the git directory ‘HumanReplayData’

\end{description}

\item {} \begin{description}
\item[{treatment: str (default=’FullLimS’)}] \leavevmode
\sphinxAtStartPar
Market treatment used. See \sphinxurl{https://papers.ssrn.com/sol3/papers.cfm?abstract\_id=3131004}

\end{description}

\item {} \begin{description}
\item[{id: int (default=954)}] \leavevmode
\sphinxAtStartPar
Player id. Must match with agent ‘role’, ‘reservation’, ‘data\_type’ and ‘treatment’. See the .csv files in
the git directory ‘HumanReplayData/data\_type’

\end{description}

\end{itemize}

\end{description}

\sphinxAtStartPar
Example of a complete agent dictionary:

\begin{sphinxVerbatim}[commandchars=\\\{\},numbers=left,firstnumber=1,stepnumber=1]
\PYG{n}{agent\PYGZus{}dict} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{sellers}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{p}{\PYGZob{}}\PYG{l+m+mi}{1}\PYG{p}{:} \PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{type}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{DQNAgent}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{reservation}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{5}\PYG{p}{\PYGZcb{}}\PYG{p}{\PYGZcb{}}\PYG{p}{,}
              \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{buyers}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{p}{\PYGZob{}}\PYG{l+m+mi}{1}\PYG{p}{:} \PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{type}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{ConstAgent}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
                             \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{reservation}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{15}\PYG{p}{,}
                             \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{const\PYGZus{}price}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{7}\PYG{p}{\PYGZcb{}}\PYG{p}{,}
                         \PYG{l+m+mi}{2}\PYG{p}{:} \PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{type}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{ConstAgent}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
                             \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{reservation}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{20}\PYG{p}{,}
                             \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{const\PYGZus{}price}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{18}\PYG{p}{\PYGZcb{}}\PYG{p}{\PYGZcb{}}\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}

\sphinxAtStartPar
This would result in one \sphinxstylestrong{DQNAgent} seller with a reservation price of 5 and two \sphinxstylestrong{ConstAgent} buyers with
reservation price and bidding price of (15, 7) and (20, 18) respectively. If all goes well, the \sphinxstylestrong{DQNAgent} should
learn to sell his product to the \sphinxstylestrong{ConstAgent} bidding 18.


\section{Initialising the environment}
\label{\detokenize{UsingTheEnvironment:initialising-the-environment}}\label{\detokenize{UsingTheEnvironment:env}}
\sphinxAtStartPar
Now we can start by initialising the environment. For this we need the following arguments:
\begin{itemize}
\item {} \begin{description}
\item[{Agent dictionary (dict)}] \leavevmode
\sphinxAtStartPar
See {\hyperref[\detokenize{UsingTheEnvironment:agent-dict}]{\sphinxcrossref{\DUrole{std,std-ref}{The agent dictionary}}}}

\end{description}

\item {} \begin{description}
\item[{Market setting (str)}] \leavevmode
\sphinxAtStartPar
You can specify what market engine to use by passing the market class name as a string. Optionally you can specify
the market engine by providing a pre\sphinxhyphen{}initialised market class object. Currently the only implemented market engine
is \sphinxstylestrong{MarketMatchHiLo}

\end{description}

\item {} \begin{description}
\item[{Information setting (str)}] \leavevmode
\sphinxAtStartPar
You can specify what information setting to use by passing the information setting class name as a string.
Optionally you can specify the information setting by providing a pre\sphinxhyphen{}initialised information setting class object.
Currently the implemented information settings are \sphinxstylestrong{BlackBoxSetting}, \sphinxstylestrong{OfferInformationSetting},
\sphinxstylestrong{DealInformationSetting} and \sphinxstylestrong{TimeInformationWrapper}

\end{description}

\item {} \begin{description}
\item[{Exploration setting (str)}] \leavevmode
\sphinxAtStartPar
You can specify what exploration setting to use by passing the exploration setting class name as a string.
Optionally you can specify the exploration setting by providing a pre\sphinxhyphen{}initialised exploration setting class object.
Currently the only implemented exploration setting is \sphinxstylestrong{LinearExplorationDecline}

\end{description}

\item {} \begin{description}
\item[{Reward setting (str)}] \leavevmode
\sphinxAtStartPar
You can specify what reward setting to use by passing the reward setting class name as a string. Optionally you can
specify the reward setting by passing a pre\sphinxhyphen{}initialised reward setting class object. Currently the only implemented
reward setting is \sphinxstylestrong{NoDealPenaltyReward}

\end{description}

\item {} \begin{description}
\item[{Optional kwargs}] \leavevmode
\sphinxAtStartPar
We can fine tune all market, information, exploration and reward settings by providing a keyword argument dictionary
for every individual setting. In addition we can specify on what device the environment and on what device the
agent networks should operate. The currently implemented keyword arguments are the following:
\begin{itemize}
\item {} \begin{description}
\item[{market\_settings}] \leavevmode\begin{itemize}
\item {} \begin{description}
\item[{Global}] \leavevmode\begin{itemize}
\item {} \begin{description}
\item[{max\_steps: int (default=30)}] \leavevmode
\sphinxAtStartPar
Maximum number of time steps before the game is reset.

\end{description}

\end{itemize}

\end{description}

\item {} \begin{description}
\item[{MarketMatchHiLo}] \leavevmode
\sphinxAtStartPar
None

\end{description}

\end{itemize}

\end{description}

\item {} \begin{description}
\item[{info\_settings}] \leavevmode\begin{itemize}
\item {} \begin{description}
\item[{Global}] \leavevmode
\sphinxAtStartPar
None

\end{description}

\item {} \begin{description}
\item[{BlackBoxSetting}] \leavevmode
\sphinxAtStartPar
None

\end{description}

\item {} \begin{description}
\item[{OfferInformationSetting}] \leavevmode\begin{itemize}
\item {} \begin{description}
\item[{n\_offers: int (default=1)}] \leavevmode
\sphinxAtStartPar
Number of offers to see. For instance, 5 would mean the agents see the best 5 bids and asks

\end{description}

\end{itemize}

\end{description}

\item {} \begin{description}
\item[{DealInformationSetting}] \leavevmode\begin{itemize}
\item {} \begin{description}
\item[{n\_deals: int (default=1)}] \leavevmode
\sphinxAtStartPar
Number of deals to see

\end{description}

\end{itemize}

\end{description}

\item {} \begin{description}
\item[{TimeInformationWrapper}] \leavevmode\begin{itemize}
\item {} \begin{description}
\item[{base\_setting: InformationSetting object (default=”BlackBoxSetting”)}] \leavevmode
\sphinxAtStartPar
The base information setting to add time to

\end{description}

\end{itemize}

\end{description}

\end{itemize}

\end{description}

\item {} \begin{description}
\item[{exploration\_settings}] \leavevmode\begin{itemize}
\item {} \begin{description}
\item[{Global}] \leavevmode
\sphinxAtStartPar
None

\end{description}

\item {} \begin{description}
\item[{LinearExplorationDecline}] \leavevmode\begin{itemize}
\item {} \begin{description}
\item[{initial\_expo: float (default=1.0)}] \leavevmode
\sphinxAtStartPar
Initial exploration probability

\end{description}

\item {} \begin{description}
\item[{n\_expo\_steps: int (default=100000)}] \leavevmode
\sphinxAtStartPar
Number of time steps over which the exploration rate will decrease linearly

\end{description}

\item {} \begin{description}
\item[{final\_expo: float (default=0.0)}] \leavevmode
\sphinxAtStartPar
Final exploration rate

\end{description}

\end{itemize}

\end{description}

\end{itemize}

\end{description}

\item {} \begin{description}
\item[{reward\_settings}] \leavevmode\begin{itemize}
\item {} \begin{description}
\item[{Global}] \leavevmode
\sphinxAtStartPar
None

\end{description}

\item {} \begin{description}
\item[{NoDealPenaltyReward}] \leavevmode\begin{itemize}
\item {} \begin{description}
\item[{no\_deal\_max: int (default=10)}] \leavevmode
\sphinxAtStartPar
Number of allowed time steps without making a deal before being punished

\end{description}

\end{itemize}

\end{description}

\end{itemize}

\end{description}

\item {} \begin{description}
\item[{device: list (default={[}‘cpu’, ‘cpu’{]})}] \leavevmode
\sphinxAtStartPar
Responsible for providing GPU support. The environment is thought to run on two GPUs. One GPU for the
environment and one for the agent optimization. If provided should be a list of two GPU devices. First device
will be for the environment, second device will be for agent networks. \sphinxstylestrong{‘cpu’} refers to the current CPU
device. \sphinxstylestrong{‘cuda’} refers to the current GPU device. In order to differentiate between different GPU devices
use \sphinxstylestrong{‘cuda:i’} where \sphinxstylestrong{i} is the respective GPU index (starting from zero)

\end{description}

\end{itemize}

\end{description}

\end{itemize}

\sphinxAtStartPar
Example of a complete keyword argument dictionary fine tuning the environment settings as well as initialising an
environment compatible with the chosen keyword arguments:

\begin{sphinxVerbatim}[commandchars=\\\{\},numbers=left,firstnumber=1,stepnumber=1]
\PYG{n}{settings\PYGZus{}kwargs} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{market\PYGZus{}settings}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{max\PYGZus{}steps}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{45}\PYG{p}{\PYGZcb{}}\PYG{p}{,}
                   \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{info\PYGZus{}settings}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{n\PYGZus{}offers}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{10}\PYG{p}{\PYGZcb{}}\PYG{p}{,}
                   \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{exploration\PYGZus{}settings}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{initial\PYGZus{}expo}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mf}{0.95}\PYG{p}{,}
                                            \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{n\PYGZus{}expo\PYGZus{}steps}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mf}{1e6}\PYG{p}{,}
                                            \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{final\PYGZus{}expo}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mf}{1e\PYGZhy{}5}\PYG{p}{\PYGZcb{}}\PYG{p}{,}
                   \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{reward\PYGZus{}settings}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{no\PYGZus{}deal\PYGZus{}max}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{5}\PYG{p}{\PYGZcb{}}\PYG{p}{,}
                   \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{device}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{cuda:0}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{cuda:1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{\PYGZcb{}}

\PYG{n}{env} \PYG{o}{=} \PYG{n}{MultiAgentEnvironment}\PYG{p}{(}\PYG{n}{agent\PYGZus{}dict}\PYG{p}{,}
                            \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{MarketMatchHiLo}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
                            \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{OfferInformationSetting}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
                            \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{LinearExplorationDecline}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
                            \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{NoDealPenaltyReward}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
                            \PYG{o}{*}\PYG{o}{*}\PYG{n}{settings\PYGZus{}kwargs}\PYG{p}{)}
\end{sphinxVerbatim}


\section{Main functionalities of the environment}
\label{\detokenize{UsingTheEnvironment:main-functionalities-of-the-environment}}
\sphinxAtStartPar
There are two main functionalities of the environment.
\begin{itemize}
\item {} \begin{description}
\item[{env.reset()}] \leavevmode
\sphinxAtStartPar
Will reset the environment to its initial settings.

\end{description}

\item {} \begin{description}
\item[{env.step(random\_action=False)}] \leavevmode
\sphinxAtStartPar
Will perform one single time step forward in the environment. If \sphinxstyleemphasis{random\_action=True} all agents will perform a
random action. In addition this function returns the current observations, current actions, current rewards,
next observations, agent states (active or finished) and a ‘done flag’ indicating if the current game has finished
or not.

\end{description}

\end{itemize}


\chapter{Quick guide to using the DeepQTrainer}
\label{\detokenize{UsingTheEnvironment:quick-guide-to-using-the-deepqtrainer}}
\sphinxAtStartPar
The \sphinxstylestrong{DeepQTrainer} is a ready built trainer copying the training procedure of the \sphinxstyleemphasis{Human\sphinxhyphen{}level control through
deep reinforcement learning} paper, see \sphinxurl{https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf} .
Therefore the application of this trainer is mainly useful in the context of training \sphinxstylestrong{DQNAgents}.


\section{Initialising the DeepQTrainer}
\label{\detokenize{UsingTheEnvironment:initialising-the-deepqtrainer}}
\sphinxAtStartPar
The main idea of this trainer, is to use a replay buffer from which samples are randomly picked in order to train the
agents. The \sphinxstylestrong{DeepQTrainer} uses the following arguments:
\begin{itemize}
\item {} \begin{description}
\item[{env: environment object}] \leavevmode
\sphinxAtStartPar
The current environment class object in which the agents are living. See {\hyperref[\detokenize{UsingTheEnvironment:env}]{\sphinxcrossref{\DUrole{std,std-ref}{Initialising the environment}}}}

\end{description}

\item {} \begin{description}
\item[{memory\_size: int}] \leavevmode
\sphinxAtStartPar
Total size of the ReplayBuffer. This corresponds to the total number of actions memorised for every agent.

\end{description}

\item {} \begin{description}
\item[{replay\_start\_size: int}] \leavevmode
\sphinxAtStartPar
Number of ReplayBuffer slots to be initialised with a uniform random policy before learning starts

\end{description}

\end{itemize}

\sphinxAtStartPar
In addition, there are some optional keyword arguments to allow for further fine tuning of the trainer
\begin{itemize}
\item {} \begin{description}
\item[{discount: float, optional (default=0.99)}] \leavevmode
\sphinxAtStartPar
Multiplicative discount factor for Q\sphinxhyphen{}learning update

\end{description}

\item {} \begin{description}
\item[{update\_frq: int, optional (default=100)}] \leavevmode
\sphinxAtStartPar
Frequency (measured in episode/game counts) with which the target network is updated

\end{description}

\item {} \begin{description}
\item[{max\_loss\_history: int, optional (default=None)}] \leavevmode
\sphinxAtStartPar
Number of previous episodes for which the loss will be saved for monitoring
None \textendash{}\textgreater{} All episode losses are saved

\end{description}

\item {} \begin{description}
\item[{max\_reward\_history: int, optional (default=None)}] \leavevmode
\sphinxAtStartPar
Number of previous episodes for which the rewards will be saved for monitoring
None \textendash{}\textgreater{} All episode rewards are saved

\end{description}

\item {} \begin{description}
\item[{max\_action\_history: int, optional (default=None)}] \leavevmode
\sphinxAtStartPar
Number of previous episodes for which the actions will be saved for monitoring
None \textendash{}\textgreater{} All episode actions are saved

\end{description}

\item {} \begin{description}
\item[{loss\_min: int, optional (default=\sphinxhyphen{}5)}] \leavevmode
\sphinxAtStartPar
Lower\sphinxhyphen{}bound for the loss to be clamped to

\end{description}

\item {} \begin{description}
\item[{loss\_max: int, optional (default=5)}] \leavevmode
\sphinxAtStartPar
Upper\sphinxhyphen{}bound for the loss to be clamped to

\end{description}

\item {} \begin{description}
\item[{save\_weights: bool, optional (default=False)}] \leavevmode
\sphinxAtStartPar
If true, all agent weights will be saved to the respective directory specified by the agent in question

\end{description}

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Example initialisation of the DeepQTrainer}

\begin{sphinxVerbatim}[commandchars=\\\{\},numbers=left,firstnumber=1,stepnumber=1]
\PYG{n}{env} \PYG{o}{=} \PYG{n}{MultiAgentEnvironment}\PYG{p}{(}\PYG{o}{.}\PYG{o}{.}\PYG{o}{.}\PYG{p}{)}
\PYG{n}{mem\PYGZus{}size} \PYG{o}{=} \PYG{l+m+mi}{10000}
\PYG{n}{start\PYGZus{}size} \PYG{o}{=} \PYG{l+m+mi}{500}

\PYG{n}{trainer} \PYG{o}{=} \PYG{n}{DeepQTrainer}\PYG{p}{(}\PYG{n}{env}\PYG{p}{,} \PYG{n}{mem\PYGZus{}size}\PYG{p}{,} \PYG{n}{start\PYGZus{}size}\PYG{p}{)}
\end{sphinxVerbatim}


\section{Training the agents}
\label{\detokenize{UsingTheEnvironment:training-the-agents}}
\sphinxAtStartPar
It is very easy to start the training process. We just need to make use of the \sphinxstylestrong{train(…)} methode. This takes the
following arguments:
\begin{itemize}
\item {} \begin{description}
\item[{n\_episodes: int}] \leavevmode
\sphinxAtStartPar
Number of episodes/games to train for

\end{description}

\item {} \begin{description}
\item[{batch\_size: int}] \leavevmode
\sphinxAtStartPar
Batch size used to update the agents network weights (Number of action/result pairs used in one learning step)

\end{description}

\end{itemize}

\sphinxAtStartPar
The trainer will return some statistics. Namely the average loss history for every agent, the average reward history
for every agent and the action history of every agent. All three are returned as a list of torch.tensors.

\sphinxAtStartPar
\sphinxstylestrong{Example use of the train(…) method}

\begin{sphinxVerbatim}[commandchars=\\\{\},numbers=left,firstnumber=1,stepnumber=1]
 \PYG{n}{n\PYGZus{}episodes} \PYG{o}{=} \PYG{l+m+mi}{100000}
 \PYG{n}{batch\PYGZus{}size} \PYG{o}{=} \PYG{l+m+mi}{32}
 \PYG{n}{total\PYGZus{}loss}\PYG{p}{,} \PYG{n}{total\PYGZus{}rew}\PYG{p}{,} \PYG{n}{actions} \PYG{o}{=} \PYG{n}{trainer}\PYG{o}{.}\PYG{n}{train}\PYG{p}{(}\PYG{n}{n\PYGZus{}episodes}\PYG{p}{,} \PYG{n}{batch\PYGZus{}size}\PYG{p}{)}
\end{sphinxVerbatim}


\section{Remark on implementing a custom training loop}
\label{\detokenize{UsingTheEnvironment:remark-on-implementing-a-custom-training-loop}}
\sphinxAtStartPar
One can implement a multitude of training loops tailored to ones custom made reinforcement learning agents.
However, a replay buffer is often needed. Therefore we slightly modified the \sphinxstylestrong{tianshou.data.ReplayBuffer} to allow
for all data to be of type \sphinxstyleemphasis{torch.tensor} and also modified the random sampling to return batches containing the
following keywords:
\begin{itemize}
\item {} \begin{description}
\item[{obs (torch.tensor)}] \leavevmode
\sphinxAtStartPar
All current observations

\end{description}

\item {} \begin{description}
\item[{act (torch.tensor)}] \leavevmode
\sphinxAtStartPar
All current actions

\end{description}

\item {} \begin{description}
\item[{rew (torch.tensor)}] \leavevmode
\sphinxAtStartPar
All current rewards

\end{description}

\item {} \begin{description}
\item[{done (bool)}] \leavevmode
\sphinxAtStartPar
Bool indicating if the episode/game has ended or not

\end{description}

\item {} \begin{description}
\item[{obs\_next (torch.tensor)}] \leavevmode
\sphinxAtStartPar
All observations from the next round (t+1)

\end{description}

\item {} \begin{description}
\item[{a\_states (torch.tensor)}] \leavevmode
\sphinxAtStartPar
All current agent states (active or done)

\end{description}

\end{itemize}

\sphinxAtStartPar
The original \sphinxstylestrong{tianshou.data.ReplayBuffer} can be found here \sphinxurl{https://github.com/thu-ml/tianshou}

\sphinxAtStartPar
The modified ReplayBuffer can be directly imported from the \sphinxstylestrong{marl\_env} directory and supports these main features

\begin{sphinxVerbatim}[commandchars=\\\{\},numbers=left,firstnumber=1,stepnumber=1]
\PYG{k+kn}{from} \PYG{n+nn}{tianshou}\PYG{n+nn}{.}\PYG{n+nn}{data} \PYG{k+kn}{import} \PYG{n}{Batch}
\PYG{k+kn}{from} \PYG{n+nn}{replay\PYGZus{}buffer} \PYG{k+kn}{import} \PYG{n}{ReplayBuffer}

\PYG{c+c1}{\PYGZsh{} Initialise the buffer with a fixed size}
\PYG{n}{buffer} \PYG{o}{=} \PYG{n}{ReplayBuffer}\PYG{p}{(}\PYG{n}{size}\PYG{o}{=}\PYG{n}{memory\PYGZus{}size}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Add a history batch to the ReplayBuffer \PYGZhy{}\PYGZhy{}\PYGZgt{} we make use of tianshou.data.Batch}
\PYG{n}{history\PYGZus{}batch} \PYG{o}{=} \PYG{n}{Batch}\PYG{p}{(}
             \PYG{n}{obs}\PYG{o}{=}\PYG{n}{obs}\PYG{p}{,}
             \PYG{n}{act}\PYG{o}{=}\PYG{n}{act}\PYG{p}{,}
             \PYG{n}{rew}\PYG{o}{=}\PYG{n}{rew}\PYG{p}{,}
             \PYG{n}{done}\PYG{o}{=}\PYG{n}{done}\PYG{p}{,}
             \PYG{n}{obs\PYGZus{}next}\PYG{o}{=}\PYG{n}{obs\PYGZus{}next}\PYG{p}{,}
             \PYG{n}{a\PYGZus{}states}\PYG{o}{=}\PYG{n}{a\PYGZus{}states}\PYG{p}{,}
         \PYG{p}{)}
\PYG{n}{buffer}\PYG{o}{.}\PYG{n}{add}\PYG{p}{(}\PYG{n}{history\PYGZus{}batch}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Sample a random minibatch of transitions from the replay buffer}
\PYG{n}{batch\PYGZus{}data}\PYG{p}{,} \PYG{n}{indices} \PYG{o}{=} \PYG{n}{buffer}\PYG{o}{.}\PYG{n}{sample}\PYG{p}{(}\PYG{n}{batch\PYGZus{}size}\PYG{o}{=}\PYG{n}{batch\PYGZus{}size}\PYG{p}{)}
\end{sphinxVerbatim}


\section{Further Usage Examples}
\label{\detokenize{UsingTheEnvironment:further-usage-examples}}
\sphinxAtStartPar
For further usage examples pleas visit the git \sphinxstylestrong{Examples} directory.


\chapter{Documentation of the code}
\label{\detokenize{MultiAgentMarketRL:documentation-of-the-code}}\label{\detokenize{MultiAgentMarketRL::doc}}

\section{A quick overview}
\label{\detokenize{MultiAgentMarketRL:a-quick-overview}}
\sphinxAtStartPar
The Multi\sphinxhyphen{}Agent\sphinxhyphen{}Market has the following components:

\sphinxAtStartPar
\sphinxstylestrong{The environment:}

\sphinxAtStartPar
The environment module connects all other modules to creat a working engine. It starts by initialising all needed
settings. These consist of all the \sphinxstyleemphasis{agents}, the \sphinxstyleemphasis{market}, the \sphinxstyleemphasis{information setting}, the \sphinxstyleemphasis{exploration setting} and the
\sphinxstyleemphasis{reward setting}. In addition further initialisation settings can be defined in an optional keyword argument dictionary.
A detailed explanation of all the environment arguments will be provided in

\sphinxAtStartPar
After initialising all agents and settings, the environment takes care of computing a single time step in the market
environment. This consists of getting all current observations, computing all agent actions, computing all the deals
which got realized at the current time step \sphinxstyleemphasis{t} as well as the associated rewards.

\sphinxAtStartPar
\sphinxstyleemphasis{Side note: The realised deals will depend on the market setting and the achieved rewards will depend on the
reward setting}.

\sphinxAtStartPar
At the end of each time step \sphinxstyleemphasis{t}, the environment will return the current observations, the
current actions, the current rewards and the current agent status (done or active) of all agents at the time step
\sphinxstyleemphasis{t} as well as the next observations of all agents at time step \sphinxstyleemphasis{t+1}. In addition the environment returns a flag
indicating if the game has finished.

\sphinxAtStartPar
\sphinxstylestrong{The market:}

\sphinxAtStartPar
The market engine is in charge of computing all realized deals at the current time step \sphinxstyleemphasis{t}.
Currently only the \sphinxstylestrong{MarketMatchHiLo} class is implemented. This market engine calculates deals by matching the
highest buying offer with the lowest selling offer. The actual deal price is then taken as the mean value between the
matches buying offer and selling offer.

\sphinxAtStartPar
\sphinxstylestrong{The agents:}

\sphinxAtStartPar
The \sphinxstylestrong{AgentSetting} class is an abstract base class for all agents. It takes care of initialising the role and
reservation price as well as the action space of each agent. Again, custom agents can be created by adding classes
overwriting specific methods of the \sphinxstylestrong{AgentSetting} class.
\begin{description}
\item[{Currently the following agents are already implemented:}] \leavevmode\begin{itemize}
\item {} \begin{description}
\item[{DQNAgent}] \leavevmode
\sphinxAtStartPar
This agent makes use of an artificial neural network in order to learn an optimal Q\sphinxhyphen{}function.
This is achieved by making use of experience replay. The Q\sphinxhyphen{}Network is then iteratively updated
using randomly selected experience minibatches.

\end{description}

\item {} \begin{description}
\item[{HumanReplayAgent}] \leavevmode
\sphinxAtStartPar
This agent replays data gathered from human experiments. All data is obtained from the
“Trading\sphinxhyphen{}in\sphinxhyphen{}a\sphinxhyphen{}Black\sphinxhyphen{}Box” repository.
\sphinxurl{https://github.com/ikicab/Trading-in-a-Black-Box/tree/f9d05b1a83882d41610638b0ceecfbb51cb05a85}

\end{description}

\item {} \begin{description}
\item[{ConstAgent}] \leavevmode
\sphinxAtStartPar
This agent will always perform the same action.

\end{description}

\end{itemize}

\end{description}

\sphinxAtStartPar
\sphinxstylestrong{The info setting}:

\sphinxAtStartPar
The information setting dictates how much information agents get before deciding on an action. Currently all agents
always have access to the same amount of information.
\begin{description}
\item[{Currently the following information settings are implemented:}] \leavevmode\begin{itemize}
\item {} \begin{description}
\item[{BlackBoxSetting}] \leavevmode
\sphinxAtStartPar
Every agent is aware of only its own last offer

\end{description}

\item {} \begin{description}
\item[{OfferInformationSetting}] \leavevmode
\sphinxAtStartPar
Every agent is aware of the best \sphinxstyleemphasis{N} offers of either side (buyer and seller) of the last
round.

\end{description}

\item {} \begin{description}
\item[{DealInformationSetting}] \leavevmode
\sphinxAtStartPar
Every agent is aware of \sphinxstyleemphasis{N} deals of the last round

\end{description}

\item {} \begin{description}
\item[{TimeInformationWrapper}] \leavevmode
\sphinxAtStartPar
Wrapper to include the current in game time in the observation.

\end{description}

\end{itemize}

\end{description}

\sphinxAtStartPar
\sphinxstylestrong{The exploration setting:}

\sphinxAtStartPar
The exploration setting determines the evolution of the probability that an agent will perform a random action (perform
exploratory actions). All agents will make use of the same exploration setting.
\begin{description}
\item[{Currently the following exploration setting is implemented:}] \leavevmode\begin{itemize}
\item {} \begin{description}
\item[{LinearExplorationDecline}] \leavevmode
\sphinxAtStartPar
Exploration probability declines linearly from an initial staring value down to a final
value over the cors of \sphinxstyleemphasis{n} steps.

\end{description}

\end{itemize}

\end{description}

\sphinxAtStartPar
\sphinxstylestrong{The reward setting:}

\sphinxAtStartPar
Calculates the reward achieved by a given agent after closing a deal
\begin{description}
\item[{Currently the following reward setting is implemented:}] \leavevmode\begin{itemize}
\item {} \begin{description}
\item[{NoDealPenaltyReward}] \leavevmode
\sphinxAtStartPar
Reward achieved by sellers is given by the difference of the deal and the reservation
price of the seller. For buyers the reward is given buy the difference of the reservation
and the deal price. In addition, buyers who spent more then N in game time steps without
making a deal will receive a linearly increasing penalty (negative reward).

\end{description}

\end{itemize}

\end{description}

\sphinxAtStartPar
\sphinxstylestrong{The trainer:}
Finally we need to be able to train agents living in a given environment. To achieve this, we can build a trainer, that
plays through multiple episodes in order to train the agents.
\begin{description}
\item[{Currently the following trainer is implemented:}] \leavevmode\begin{itemize}
\item {} \begin{description}
\item[{DeepQTrainer}] \leavevmode
\sphinxAtStartPar
Trainer following the philosophy of the DQN agents.

\end{description}

\end{itemize}

\end{description}


\section{The Agent Module}
\label{\detokenize{MultiAgentMarketRL:the-agent-module}}\begin{description}
\item[{The \sphinxstyleemphasis{AgentSetting} class provides the basic building blocks required by all agents. This includes the following::}] \leavevmode\begin{itemize}
\item {} \begin{description}
\item[{Basic initialisation of the agent}] \leavevmode
\sphinxAtStartPar
Every agent needs a role (buyer or seller), a reservation price, an observation space and an action space.
The observation space is automatically determined according to the chosen information setting. The action space
is automatically determined according to the agent role as well as the reservation prices of all other agents.
Lastly every agent can optionally be assigned to a specific device (cpu or gpu).

\end{description}

\item {} \begin{description}
\item[{All necessary methods (in the context of the DeepQTrainer)}] \leavevmode
\sphinxAtStartPar
The methods \sphinxstyleemphasis{get\_action}, \sphinxstyleemphasis{random\_action}, \sphinxstyleemphasis{get\_q\_value}, \sphinxstyleemphasis{get\_target}, \sphinxstyleemphasis{reset\_target\_network},
\sphinxstyleemphasis{save\_model\_weights} and \sphinxstyleemphasis{load\_model\_weights} all need to be callable in the context of the DeepQTrainer.
However, their functionality is highly agent dependent. Therefore all methods implemented in the \sphinxstyleemphasis{AgentSetting}
base class will raise \sphinxstyleemphasis{NotImplementedError}. Therefore every agent class has to overwrite the \sphinxstyleemphasis{get\_action}
method by means of inheritance of the \sphinxstyleemphasis{AgentSetting} class and thereby assigning the wanted behaviour of each
method.

\end{description}

\end{itemize}

\end{description}


\subsection{Ready to use agents}
\label{\detokenize{MultiAgentMarketRL:ready-to-use-agents}}
\sphinxAtStartPar
The \sphinxstylestrong{DQNAgent} class represents an agent who aims to approximate the optimal Q\sphinxhyphen{}function via a neural network. This is
done by having two identically initialised neural networks (the Q\sphinxhyphen{}network and the target\sphinxhyphen{}network). When learning, the
Q\sphinxhyphen{}network is updated accordingly to the ‘ground truths’ provided by the target\sphinxhyphen{}network. Periodically the target\sphinxhyphen{}network
is then reinitialised with the new weights from the Q\sphinxhyphen{}network.

\sphinxAtStartPar
The \sphinxstylestrong{HumanReplayAgent} class represents an agent capable of replaying human data gathered in a study. See this paper
\sphinxurl{https://papers.ssrn.com/sol3/papers.cfm?abstract\_id=3131004} and/or the git directory ‘HumanReplayData’ containing the
the data as well as additional information such as player ids and reservation prices…

\sphinxAtStartPar
The \sphinxstylestrong{ConstAgent} class represents an agent who bids/asks a constant price during the entire game.


\subsection{Creating your custom agent}
\label{\detokenize{MultiAgentMarketRL:creating-your-custom-agent}}
\sphinxAtStartPar
It is very easy to add your custom agent to the \sphinxstylestrong{agents.py} file. This is done in the following way

\begin{sphinxVerbatim}[commandchars=\\\{\},numbers=left,firstnumber=1,stepnumber=1]
\PYG{k}{class} \PYG{n+nc}{MyCustomAgent}\PYG{p}{(}\PYG{n}{AgentSetting}\PYG{p}{)}\PYG{p}{:}
   \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,}
                \PYG{n}{role}\PYG{p}{,}
                \PYG{n}{reservation}\PYG{p}{,}
                \PYG{n}{in\PYGZus{}features}\PYG{p}{,}
                \PYG{n}{action\PYGZus{}boundary}\PYG{p}{,}
                \PYG{n}{device}\PYG{o}{=}\PYG{n}{torch}\PYG{o}{.}\PYG{n}{device}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{cpu}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{,}
                \PYG{o}{*}\PYG{o}{*}\PYG{n}{kwargs}\PYG{p}{)}\PYG{p}{:}

      \PYG{n+nb}{super}\PYG{p}{(}\PYG{n}{MyCustomAgent}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{o}{.}\PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n}{role}\PYG{p}{,}
                                          \PYG{n}{reservation}\PYG{p}{,}
                                          \PYG{n}{in\PYGZus{}features}\PYG{p}{,}
                                          \PYG{n}{action\PYGZus{}boundary}\PYG{p}{,}
                                          \PYG{n}{device}\PYG{o}{=}\PYG{n}{device}\PYG{p}{)}

      \PYG{o}{\PYGZlt{}}\PYG{n}{your} \PYG{n}{custom} \PYG{n}{code}\PYG{o}{\PYGZgt{}}

   \PYG{k}{def} \PYG{n+nf}{get\PYGZus{}action}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{observation}\PYG{p}{,} \PYG{n}{epsilon}\PYG{o}{=}\PYG{l+m+mf}{0.05}\PYG{p}{)}\PYG{p}{:}
      \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{      Parameters}
\PYG{l+s+sd}{      \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}}
\PYG{l+s+sd}{      observation: torch.tensor}
\PYG{l+s+sd}{         Current observations}
\PYG{l+s+sd}{      epsilon: float, obtional (default=0.05)}
\PYG{l+s+sd}{         Probability for a random action}

\PYG{l+s+sd}{      Returns}
\PYG{l+s+sd}{      \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}}
\PYG{l+s+sd}{      action: torch.tensor}
\PYG{l+s+sd}{      \PYGZdq{}\PYGZdq{}\PYGZdq{}}


      \PYG{o}{\PYGZlt{}}\PYG{n}{your} \PYG{n}{custom} \PYG{n}{code}\PYG{o}{\PYGZgt{}}

      \PYG{k}{return} \PYG{n}{action}

   \PYG{o}{.}\PYG{o}{.}\PYG{o}{.}
\end{sphinxVerbatim}

\sphinxAtStartPar
Every agent class that inherits from the \sphinxstylestrong{AgentSetting} class will have the following methods
\begin{itemize}
\item {} \begin{description}
\item[{get\_action(self, observation, epsilon=0.05)}] \leavevmode
\sphinxAtStartPar
observation: torch.tensor, epsilon: float

\end{description}

\item {} \begin{description}
\item[{random\_action(self, observation=None, epsilon=None)}] \leavevmode
\sphinxAtStartPar
observation: torch.tensor, epsilon: float

\end{description}

\item {} \begin{description}
\item[{get\_q\_value(self, observation, actions=None)}] \leavevmode
\sphinxAtStartPar
observation: torch.tensor, action: torch.tensor

\end{description}

\item {} \begin{description}
\item[{get\_target(self, observation, agent\_state=None)}] \leavevmode
\sphinxAtStartPar
observation: torch.tensor, action: torch.tensor

\end{description}

\item {} \begin{description}
\item[{reset\_target\_network(self)}] \leavevmode
\sphinxAtStartPar
No arguments

\end{description}

\item {} \begin{description}
\item[{save\_model\_weights(self)}] \leavevmode
\sphinxAtStartPar
No arguments

\end{description}

\end{itemize}

\sphinxAtStartPar
By default all methods will raise \sphinxstyleemphasis{NotImplementedError}. In order for your custom agent to make proper use of these
methods, you will have to overwrite them. This is achieved by defining the exact same function in your custom agent
class as was done for the \sphinxstylestrong{get\_action} method in the code example above. See the raw methods documentation for
detailed description of input and output types.

\sphinxAtStartPar
\sphinxstylestrong{Side note:} Not all agent types will necessarily make use of all methods. For instance zero intelligence agents will
never have to save weights. In order to circumvent the need to distinguish between agent types during training, we
suggest nonetheless implementing the ‘useless’ method as a dummy function consisting of a \sphinxstyleemphasis{pass} statement.

\sphinxAtStartPar
Furthermore all intelligent agents can make use of your custom neural network model via the \sphinxstylestrong{NetworkSetting} class.
First, you can define your own neural network model in the \sphinxstylestrong{network\_models.py} file by inheriting the
\sphinxstylestrong{NetworkSetting} base class and overwriting the \sphinxstylestrong{define\_network()} method. Be sure to return a pytorch neural
network model.

\begin{sphinxVerbatim}[commandchars=\\\{\},numbers=left,firstnumber=1,stepnumber=1]
\PYG{k}{class} \PYG{n+nc}{MyCustomNetwork}\PYG{p}{(}\PYG{n}{NetworkSetting}\PYG{p}{)}\PYG{p}{:}
   \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{in\PYGZus{}features}\PYG{p}{,} \PYG{n}{out\PYGZus{}features}\PYG{p}{,} \PYG{n}{device}\PYG{o}{=}\PYG{n}{torch}\PYG{o}{.}\PYG{n}{device}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{cpu}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{,} \PYG{o}{*}\PYG{o}{*}\PYG{n}{kwargs}\PYG{p}{)}\PYG{p}{:}
      \PYG{n+nb}{super}\PYG{p}{(}\PYG{n}{MyCustomNetwork}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{o}{.}\PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n}{in\PYGZus{}features}\PYG{p}{,}
                                            \PYG{n}{out\PYGZus{}features}\PYG{p}{,}
                                            \PYG{n}{device}\PYG{o}{=}\PYG{n}{device}\PYG{p}{,}
                                            \PYG{o}{*}\PYG{o}{*}\PYG{n}{kwargs}\PYG{p}{)}

   \PYG{k}{def} \PYG{n+nf}{define\PYGZus{}network}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{p}{:}
   \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{   Defines a simple network for the purpose of being an example}

\PYG{l+s+sd}{   Returns}
\PYG{l+s+sd}{   \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}}
\PYG{l+s+sd}{   network: torch.nn}
\PYG{l+s+sd}{      The wanted neural network}
\PYG{l+s+sd}{   \PYGZdq{}\PYGZdq{}\PYGZdq{}}
   \PYG{n}{network} \PYG{o}{=} \PYG{o}{\PYGZlt{}}\PYG{n}{your} \PYG{n}{custom} \PYG{n}{torch} \PYG{n}{neural} \PYG{n}{network}\PYG{o}{\PYGZgt{}}

   \PYG{k}{return} \PYG{n}{network}
\end{sphinxVerbatim}

\sphinxAtStartPar
In order to load your custom neural network into your intelligent agent, be sure to add a keyword argument
\sphinxcode{\sphinxupquote{"network\_type"="MyCustomNetwork"}} to your agent kwargs dictionary, where “MyCustomNetwork” is a placeholder for the class
name of your actual network. Now you can load the neural network as follows:

\begin{sphinxVerbatim}[commandchars=\\\{\},numbers=left,firstnumber=1,stepnumber=1]
\PYG{k+kn}{import} \PYG{n+nn}{network\PYGZus{}models} \PYG{k}{as} \PYG{n+nn}{network\PYGZus{}models}

\PYG{k}{class} \PYG{n+nc}{MyCustomAgent}\PYG{p}{(}\PYG{n}{AgentSetting}\PYG{p}{)}\PYG{p}{:}
   \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{role}\PYG{p}{,} \PYG{n}{reservation}\PYG{p}{,} \PYG{n}{in\PYGZus{}features}\PYG{p}{,} \PYG{o}{.}\PYG{o}{.}\PYG{o}{.}\PYG{p}{,} \PYG{n}{device}\PYG{p}{,} \PYG{o}{*}\PYG{o}{*}\PYG{n}{kwargs}\PYG{p}{)}\PYG{p}{:}
      \PYG{o}{.}\PYG{o}{.}\PYG{o}{.}
      \PYG{n}{network\PYGZus{}type} \PYG{o}{=} \PYG{n}{kwargs}\PYG{o}{.}\PYG{n}{pop}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{network\PYGZus{}type}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{o}{\PYGZlt{}}\PYG{n}{name} \PYG{n}{of} \PYG{n}{default} \PYG{n}{network}\PYG{o}{\PYGZgt{}}\PYG{p}{)}
      \PYG{n}{out\PYGZus{}features} \PYG{o}{=} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{action\PYGZus{}space}\PYG{p}{)}
      \PYG{n}{network\PYGZus{}builder} \PYG{o}{=} \PYG{n+nb}{getattr}\PYG{p}{(}\PYG{n}{network\PYGZus{}models}\PYG{p}{,} \PYG{n}{network\PYGZus{}type}\PYG{p}{)}\PYG{p}{(}
         \PYG{n}{in\PYGZus{}features}\PYG{p}{,} \PYG{n}{out\PYGZus{}features}\PYG{p}{,} \PYG{n}{device}\PYG{o}{=}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{device}\PYG{p}{,} \PYG{o}{*}\PYG{o}{*}\PYG{n}{kwargs}
      \PYG{p}{)}

      \PYG{n}{my\PYGZus{}custom\PYGZus{}network} \PYG{o}{=} \PYG{n}{network\PYGZus{}builder}\PYG{o}{.}\PYG{n}{get\PYGZus{}network}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
The resulting network will already be located on the correct device (cpu or gpu). Also, if kwargs contains a
“load\_weights\_path” keyword, the model weights will be loaded from the provided path.

\sphinxAtStartPar
Now you can make use of your custom agent in the same way as you use the other agents.


\section{The Market Module}
\label{\detokenize{MultiAgentMarketRL:the-market-module}}
\sphinxAtStartPar
The \sphinxstylestrong{BaseMarketEngine} class forms the basis of all market engines. It takes care of initialising the number of
sellers and the number of buyers as well as the max time duration of one episode/game and implements the buyer, seller
and deal histories.


\subsection{Ready to use market engines}
\label{\detokenize{MultiAgentMarketRL:ready-to-use-market-engines}}
\sphinxAtStartPar
The \sphinxstylestrong{MarketMatchHiLo} class is a ready implemented matching algorithm matching the highest bidding buyer with the
lowest asking seller, second highest bidder with the second lowest asking seller, and so on. The actual deal price is
then computed as the mean between the matches bidding and asking prices.


\subsection{Creating your custom engine}
\label{\detokenize{MultiAgentMarketRL:creating-your-custom-engine}}
\sphinxAtStartPar
Again you can creat your own market engine with custom deal matching by inheriting the \sphinxstylestrong{BaseMarketEngine} class and
overwriting the \sphinxstylestrong{calculate\_deals} method. However you are confined to returning the calculated deals for sellers and
buyers separately as torch.tensors.

\begin{sphinxVerbatim}[commandchars=\\\{\},numbers=left,firstnumber=1,stepnumber=1]
\PYG{k}{class} \PYG{n+nc}{MyCustomMarketEngine}\PYG{p}{(}\PYG{n}{BaseMarketEngine}\PYG{p}{)}\PYG{p}{:}

   \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{n\PYGZus{}sellers}\PYG{p}{,} \PYG{n}{n\PYGZus{}buyers}\PYG{p}{,} \PYG{n}{device}\PYG{o}{=}\PYG{n}{torch}\PYG{o}{.}\PYG{n}{device}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{cpu}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{,} \PYG{o}{*}\PYG{o}{*}\PYG{n}{kwargs}\PYG{p}{)}\PYG{p}{:}
      \PYG{n+nb}{super}\PYG{p}{(}\PYG{n}{MyCustomMarketEngine}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{o}{.}\PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n}{n\PYGZus{}sellers}\PYG{p}{,}
                                                 \PYG{n}{n\PYGZus{}buyers}\PYG{p}{,}
                                                 \PYG{n}{device}\PYG{o}{=}\PYG{n}{device}\PYG{p}{,}
                                                 \PYG{o}{*}\PYG{o}{*}\PYG{n}{kwargs}\PYG{p}{)}

      \PYG{o}{\PYGZlt{}}\PYG{n}{your} \PYG{n}{custom} \PYG{n}{code}\PYG{o}{\PYGZgt{}}

   \PYG{k}{def} \PYG{n+nf}{calculate\PYGZus{}deals}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{s\PYGZus{}actions}\PYG{p}{,} \PYG{n}{b\PYGZus{}actions}\PYG{p}{)}\PYG{p}{:}
      \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}

\PYG{l+s+sd}{      Parameters}
\PYG{l+s+sd}{      \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}}
\PYG{l+s+sd}{      s\PYGZus{}actions: torch.Tensor}
\PYG{l+s+sd}{         Has shape n\PYGZus{}sellers}
\PYG{l+s+sd}{      b\PYGZus{}actions: torch.Tensor}
\PYG{l+s+sd}{         Has shape n\PYGZus{}buyers}

\PYG{l+s+sd}{      Returns}
\PYG{l+s+sd}{      \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}}
\PYG{l+s+sd}{      deals\PYGZus{}sellers: torch.Tensor}
\PYG{l+s+sd}{         Has shape n\PYGZus{}sellers}
\PYG{l+s+sd}{      deals\PYGZus{}buyers: torch.Tensor}
\PYG{l+s+sd}{         Has shape n\PYGZus{}buyers}
\PYG{l+s+sd}{      \PYGZdq{}\PYGZdq{}\PYGZdq{}}

      \PYG{o}{\PYGZlt{}}\PYG{n}{your} \PYG{n}{custom} \PYG{n}{code}\PYG{o}{\PYGZgt{}}

      \PYG{k}{return} \PYG{n}{deals\PYGZus{}sellers}\PYG{p}{,} \PYG{n}{deals\PYGZus{}buyers}
\end{sphinxVerbatim}

\sphinxAtStartPar
Again, once you have implemented you custom market engine in the \sphinxstyleemphasis{markets.py} file, you can make use of the new market
engine as usual in the environment.


\section{The Information Setting Module}
\label{\detokenize{MultiAgentMarketRL:the-information-setting-module}}
\sphinxAtStartPar
The \sphinxstylestrong{InformationSetting} class forms the basis of all information settings. It provides access to all environment
variables as well as the \sphinxstylestrong{get\_states(…)} method which can be overwritten in order to define your custom information
states.


\subsection{Ready to use information settings}
\label{\detokenize{MultiAgentMarketRL:ready-to-use-information-settings}}
\sphinxAtStartPar
The \sphinxstylestrong{BlackBoxSetting} class provides a setting where the information state of each agent consists of only its last
action.

\sphinxAtStartPar
The \sphinxstylestrong{OfferInformationSetting} class provides a setting where the information state of each agent consists of the best
N offers on both sides (buyer and seller). Therefore each agent will have the same 2*N offers as its information state.

\sphinxAtStartPar
The \sphinxstylestrong{DealInformationSetting} class provides a setting where the information state of each agent consists of the best
N deals of the last round. Again, every agent will have the same information state.

\sphinxAtStartPar
The \sphinxstylestrong{TimeInformationWrapper} class takes any other information setting class and adds the current time step to the
information state.


\subsection{Creating your custom information setting}
\label{\detokenize{MultiAgentMarketRL:creating-your-custom-information-setting}}
\sphinxAtStartPar
As stated above, you can creat your custom information setting by inheriting the \sphinxstylestrong{InformationSetting} base class and
overwriting the \sphinxstylestrong{get\_states(…)} method. Again, the return object is the limiting factor to your creativity. It is
expected, that the \sphinxstylestrong{get\_states(…)} returns a \sphinxstyleemphasis{torch.tensor} object of size (n\_agents, n\_features). Where the
n\_agents dimension first contains all sellers and then contains all buyers. n\_features represents the number of
distinct information feature each agent gets.

\begin{sphinxVerbatim}[commandchars=\\\{\},numbers=left,firstnumber=1,stepnumber=1]
\PYG{k}{class} \PYG{n+nc}{MyCustomInformationClass}\PYG{p}{(}\PYG{n}{InformationSetting}\PYG{p}{)}\PYG{p}{:}

       \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{env}\PYG{p}{,} \PYG{o}{*}\PYG{o}{*}\PYG{n}{kwargs}\PYG{p}{)}\PYG{p}{:}
           \PYG{n+nb}{super}\PYG{p}{(}\PYG{n}{MyCustomInformationClass}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{o}{.}\PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n}{env}\PYG{p}{)}

           \PYG{o}{\PYGZlt{}}\PYG{n}{your} \PYG{n}{custom} \PYG{n}{code}\PYG{o}{\PYGZgt{}}

       \PYG{k}{def} \PYG{n+nf}{get\PYGZus{}states}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{p}{:}
           \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{           Returns}
\PYG{l+s+sd}{           \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}}
\PYG{l+s+sd}{           total\PYGZus{}info: torch.tensor}
\PYG{l+s+sd}{               Return total\PYGZus{}info as tensor with shape (n\PYGZus{}agents, n\PYGZus{}features) where}
\PYG{l+s+sd}{               n\PYGZus{}features == number of infos Observations are ordered in the same way}
\PYG{l+s+sd}{               as res in MultiAgentEnvironment.get\PYGZus{}actions().}
\PYG{l+s+sd}{               total\PYGZus{}info[:n\PYGZus{}sellers, :] contains all observations for the seller agents}
\PYG{l+s+sd}{               total\PYGZus{}info[n\PYGZus{}sellers:, :] contains all observations for the buyer agents}
\PYG{l+s+sd}{           \PYGZdq{}\PYGZdq{}\PYGZdq{}}

           \PYG{o}{\PYGZlt{}}\PYG{n}{your} \PYG{n}{custom} \PYG{n}{code}\PYG{o}{\PYGZgt{}}

           \PYG{k}{return} \PYG{n}{total\PYGZus{}info}
\end{sphinxVerbatim}

\sphinxAtStartPar
Again, once you have implemented your custom information setting in the \sphinxstylestrong{info\_setting.py}, you can make use of it in
the same way as the ready to use info settings.


\section{The Exploration Setting Module}
\label{\detokenize{MultiAgentMarketRL:the-exploration-setting-module}}
\sphinxAtStartPar
The \sphinxstylestrong{ExplorationSetting} class forms the base class of all exploration settings. Currently it does absolutely nothing
besides initialising the \sphinxstyleemphasis{epsilon} class variable representing the probability for a random (exploratory) action.


\subsection{Ready to use exploration settings}
\label{\detokenize{MultiAgentMarketRL:ready-to-use-exploration-settings}}
\sphinxAtStartPar
The \sphinxstylestrong{LinearExplorationDecline} class provides a setting, where the exploration vale declines linearly from a given
starting value to a given final value over a given number of steps.


\subsection{Creating your custom exploration setting}
\label{\detokenize{MultiAgentMarketRL:creating-your-custom-exploration-setting}}
\sphinxAtStartPar
As always, you can creat your custom exploration setting in the \sphinxstylestrong{exploration\_setting.py} file by inheriting from the
\sphinxstylestrong{ExplorationSetting} base class and overwriting the \sphinxstylestrong{update()} method. Again, the ‘updated’ probability to perform
a random action must be assigned to the \sphinxstyleemphasis{epsilon} class variable in order to have an effect.

\begin{sphinxVerbatim}[commandchars=\\\{\},numbers=left,firstnumber=1,stepnumber=1]
\PYG{k}{class} \PYG{n+nc}{MyCustomExploration}\PYG{p}{(}\PYG{n}{ExplorationSetting}\PYG{p}{)}\PYG{p}{:}

   \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{o}{*}\PYG{o}{*}\PYG{n}{kwargs}\PYG{p}{)}\PYG{p}{:}
      \PYG{n+nb}{super}\PYG{p}{(}\PYG{n}{MyCustomExploration}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{o}{.}\PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{o}{*}\PYG{o}{*}\PYG{n}{kwargs}\PYG{p}{)}

      \PYG{o}{\PYGZlt{}}\PYG{n}{your} \PYG{n}{custom} \PYG{n}{code}\PYG{o}{\PYGZgt{}}

   \PYG{k}{def} \PYG{n+nf}{update}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{p}{:}

      \PYG{o}{\PYGZlt{}}\PYG{n}{your} \PYG{n}{custom} \PYG{n}{code}\PYG{o}{\PYGZgt{}}

      \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{epsilon} \PYG{o}{=} \PYG{o}{\PYGZlt{}}\PYG{n}{your} \PYG{n}{new} \PYG{n}{epsilon} \PYG{n}{value}\PYG{o}{\PYGZgt{}}
\end{sphinxVerbatim}

\sphinxAtStartPar
Now you can make use of your newly made exploration setting in the same way as the ready made ones can be implemented
into the environment.


\section{The Reward Setting Module}
\label{\detokenize{MultiAgentMarketRL:the-reward-setting-module}}
\sphinxAtStartPar
The \sphinxstylestrong{RewardSetting} class forms the basis of all reward settings. It provides access to all environment variables.


\subsection{Ready to use reward settings}
\label{\detokenize{MultiAgentMarketRL:ready-to-use-reward-settings}}
\sphinxAtStartPar
The \sphinxstylestrong{NoDealPenaltyReward} class provides a reward setting, where buyers receive a linearly growing penalty if they do
not manage to close a deal after N rounds. This has the objective of enticing the agents to act quickly.


\subsection{Creating your custom reward setting}
\label{\detokenize{MultiAgentMarketRL:creating-your-custom-reward-setting}}
\sphinxAtStartPar
In order to creat your custom reward setting in the \sphinxstylestrong{reward\_setting.py} file, you must inherit the \sphinxstylestrong{RewardSetting}
base class and overwrite the \sphinxstylestrong{seller\_reward()} and the \sphinxstylestrong{buyer\_reward} methods. Again, you are restricted in the
shape of the return object. Both methods need to return a torch.tensor object with shape (n\_sellers,) and (n\_buyers,)
respectively.

\begin{sphinxVerbatim}[commandchars=\\\{\},numbers=left,firstnumber=1,stepnumber=1]
\PYG{k}{class} \PYG{n+nc}{MyCustomReward}\PYG{p}{(}\PYG{n}{RewardSetting}\PYG{p}{)}\PYG{p}{:}

   \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{env}\PYG{p}{,} \PYG{o}{*}\PYG{o}{*}\PYG{n}{kwargs}\PYG{p}{)}\PYG{p}{:}
      \PYG{n+nb}{super}\PYG{p}{(}\PYG{n}{MyCustomReward}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{o}{.}\PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n}{env}\PYG{p}{)}

      \PYG{o}{\PYGZlt{}}\PYG{n}{your} \PYG{n}{custom} \PYG{n}{code}\PYG{o}{\PYGZgt{}}

   \PYG{k}{def} \PYG{n+nf}{seller\PYGZus{}reward}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{seller\PYGZus{}deals}\PYG{p}{)}\PYG{p}{:}
      \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{      Parameters}
\PYG{l+s+sd}{      \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}}
\PYG{l+s+sd}{      seller\PYGZus{}deals: torch.tensor}
\PYG{l+s+sd}{         Has shape (n\PYGZus{}sellers,)}

\PYG{l+s+sd}{      Returns}
\PYG{l+s+sd}{      \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}}
\PYG{l+s+sd}{      seller\PYGZus{}rewards: torch.tensor}
\PYG{l+s+sd}{         Has shape (n\PYGZus{}sellers,)}
\PYG{l+s+sd}{      \PYGZdq{}\PYGZdq{}\PYGZdq{}}

      \PYG{o}{\PYGZlt{}}\PYG{n}{your} \PYG{n}{custom} \PYG{n}{code}\PYG{o}{\PYGZgt{}}

      \PYG{k}{return} \PYG{n}{seller\PYGZus{}rewards}

   \PYG{k}{def} \PYG{n+nf}{buyer\PYGZus{}reward}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{buyer\PYGZus{}deals}\PYG{p}{)}\PYG{p}{:}
      \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{      Parameters}
\PYG{l+s+sd}{      \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}}
\PYG{l+s+sd}{      buyer\PYGZus{}deals: torch.tensor}
\PYG{l+s+sd}{         Has shape (n\PYGZus{}buyers,)}

\PYG{l+s+sd}{      Returns}
\PYG{l+s+sd}{      \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}}
\PYG{l+s+sd}{      buyer\PYGZus{}rewards: torch.tensor}
\PYG{l+s+sd}{         Has shape (n\PYGZus{}buyers,)}
\PYG{l+s+sd}{      \PYGZdq{}\PYGZdq{}\PYGZdq{}}

      \PYG{o}{\PYGZlt{}}\PYG{n}{your} \PYG{n}{custom} \PYG{n}{code}\PYG{o}{\PYGZgt{}}

      \PYG{k}{return} \PYG{n}{buyer\PYGZus{}rewards}
\end{sphinxVerbatim}

\sphinxAtStartPar
Now you can make use of your custom reward setting in the same manner as the ready implemented reward settings are used.


\section{The Neural Network Model Module}
\label{\detokenize{MultiAgentMarketRL:the-neural-network-model-module}}
\sphinxAtStartPar
The \sphinxstylestrong{NetworkSetting} class is intended to provide easy to use modularity with regards to the neural networks used by
intelligent agents. Following the same architecture as the previous modules, the \sphinxstylestrong{NetworkSetting} class forms the base
class, providing all needed functionalities.


\subsection{Ready to use network models}
\label{\detokenize{MultiAgentMarketRL:ready-to-use-network-models}}
\sphinxAtStartPar
The \sphinxstylestrong{SimpleExampleNetwork} class provides a simple neural network for the purpose of being an example.


\subsection{Creating your custom neural network}
\label{\detokenize{MultiAgentMarketRL:creating-your-custom-neural-network}}
\sphinxAtStartPar
In order to define your own neural network in the \sphinxstylestrong{network\_models.py} file, which can be used by intelligent agents,
you have to inherit the \sphinxstylestrong{NetworkSetting} base class and overwrite the \sphinxstylestrong{define\_network} methode. The class variables
\sphinxstylestrong{in\_features} and \sphinxstylestrong{out\_features} provide the input and output shape of your neural network. The method must return
a torch neural network (torch.nn).

\begin{sphinxVerbatim}[commandchars=\\\{\},numbers=left,firstnumber=1,stepnumber=1]
\PYG{k}{class} \PYG{n+nc}{MyCustomNetworkModel}\PYG{p}{(}\PYG{n}{NetworkSetting}\PYG{p}{)}\PYG{p}{:}

   \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{in\PYGZus{}features}\PYG{p}{,} \PYG{n}{out\PYGZus{}features}\PYG{p}{,} \PYG{n}{device}\PYG{o}{=}\PYG{n}{torch}\PYG{o}{.}\PYG{n}{device}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{cpu}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{,} \PYG{o}{*}\PYG{o}{*}\PYG{n}{kwargs}\PYG{p}{)}\PYG{p}{:}
      \PYG{n+nb}{super}\PYG{p}{(}\PYG{n}{SimpleExampleNetwork}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{o}{.}\PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n}{in\PYGZus{}features}\PYG{p}{,}
                                                 \PYG{n}{out\PYGZus{}features}\PYG{p}{,}
                                                 \PYG{n}{device}\PYG{o}{=}\PYG{n}{device}\PYG{p}{,}
                                                 \PYG{o}{*}\PYG{o}{*}\PYG{n}{kwargs}\PYG{p}{)}

   \PYG{k}{def} \PYG{n+nf}{define\PYGZus{}network}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{p}{:}
      \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{      Returns}
\PYG{l+s+sd}{      \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}}
\PYG{l+s+sd}{      network: torch.nn}
\PYG{l+s+sd}{         The wanted neural network}
\PYG{l+s+sd}{      \PYGZdq{}\PYGZdq{}\PYGZdq{}}

      \PYG{o}{\PYGZlt{}}\PYG{n}{your} \PYG{n}{custom} \PYG{n}{code}\PYG{o}{\PYGZgt{}}

      \PYG{k}{return} \PYG{n}{network}
\end{sphinxVerbatim}

\sphinxAtStartPar
Now you can import your newly created neural network model into your custom made agent class by making use of the
\sphinxstylestrong{get\_network} methode. Add something in the lines of this to your init function:

\begin{sphinxVerbatim}[commandchars=\\\{\},numbers=left,firstnumber=1,stepnumber=1]
\PYG{n}{network\PYGZus{}builder} \PYG{o}{=} \PYG{n+nb}{getattr}\PYG{p}{(}\PYG{n}{network\PYGZus{}models}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{MyCustomNetworkModel}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{(}
    \PYG{n}{in\PYGZus{}features}\PYG{p}{,} \PYG{n}{out\PYGZus{}features}\PYG{p}{,} \PYG{n}{device}\PYG{o}{=}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{device}\PYG{p}{,} \PYG{o}{*}\PYG{o}{*}\PYG{n}{kwargs}
\PYG{p}{)}

\PYG{n}{my\PYGZus{}network} \PYG{o}{=} \PYG{n}{network\PYGZus{}builder}\PYG{o}{.}\PYG{n}{get\PYGZus{}network}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}


\section{Raw module functions}
\label{\detokenize{MultiAgentMarketRL:module-agents}}\label{\detokenize{MultiAgentMarketRL:raw-module-functions}}\index{module@\spxentry{module}!agents@\spxentry{agents}}\index{agents@\spxentry{agents}!module@\spxentry{module}}\index{AgentSetting (class in agents)@\spxentry{AgentSetting}\spxextra{class in agents}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{MultiAgentMarketRL:agents.AgentSetting}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{agents.}}\sphinxbfcode{\sphinxupquote{AgentSetting}}}{\emph{\DUrole{n}{role}}, \emph{\DUrole{n}{reservation}}, \emph{\DUrole{n}{in\_features}}, \emph{\DUrole{n}{action\_boundary}}, \emph{\DUrole{n}{device}\DUrole{o}{=}\DUrole{default_value}{device(type=\textquotesingle{}cpu\textquotesingle{})}}}{}
\sphinxAtStartPar
Abstract agent class
\index{\_\_init\_\_() (agents.AgentSetting method)@\spxentry{\_\_init\_\_()}\spxextra{agents.AgentSetting method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{MultiAgentMarketRL:agents.AgentSetting.__init__}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{\_\_init\_\_}}}{\emph{\DUrole{n}{role}}, \emph{\DUrole{n}{reservation}}, \emph{\DUrole{n}{in\_features}}, \emph{\DUrole{n}{action\_boundary}}, \emph{\DUrole{n}{device}\DUrole{o}{=}\DUrole{default_value}{device(type=\textquotesingle{}cpu\textquotesingle{})}}}{{ $\rightarrow$ None}}
\end{fulllineitems}

\index{get\_action() (agents.AgentSetting method)@\spxentry{get\_action()}\spxextra{agents.AgentSetting method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{MultiAgentMarketRL:agents.AgentSetting.get_action}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{get\_action}}}{\emph{\DUrole{n}{observation}}, \emph{\DUrole{n}{epsilon}\DUrole{o}{=}\DUrole{default_value}{0.05}}}{{ $\rightarrow$ NotImplementedError}}
\end{fulllineitems}

\index{get\_q\_value() (agents.AgentSetting method)@\spxentry{get\_q\_value()}\spxextra{agents.AgentSetting method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{MultiAgentMarketRL:agents.AgentSetting.get_q_value}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{get\_q\_value}}}{\emph{\DUrole{n}{observation}}, \emph{\DUrole{n}{actions}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ NotImplementedError}}
\end{fulllineitems}

\index{get\_target() (agents.AgentSetting method)@\spxentry{get\_target()}\spxextra{agents.AgentSetting method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{MultiAgentMarketRL:agents.AgentSetting.get_target}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{get\_target}}}{\emph{\DUrole{n}{observation}}, \emph{\DUrole{n}{agent\_state}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ NotImplementedError}}
\end{fulllineitems}

\index{random\_action() (agents.AgentSetting method)@\spxentry{random\_action()}\spxextra{agents.AgentSetting method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{MultiAgentMarketRL:agents.AgentSetting.random_action}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{random\_action}}}{\emph{\DUrole{n}{observation}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{epsilon}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ NotImplementedError}}
\end{fulllineitems}

\index{reset\_target\_network() (agents.AgentSetting method)@\spxentry{reset\_target\_network()}\spxextra{agents.AgentSetting method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{MultiAgentMarketRL:agents.AgentSetting.reset_target_network}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{reset\_target\_network}}}{}{{ $\rightarrow$ NotImplementedError}}
\end{fulllineitems}

\index{save\_model\_weights() (agents.AgentSetting method)@\spxentry{save\_model\_weights()}\spxextra{agents.AgentSetting method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{MultiAgentMarketRL:agents.AgentSetting.save_model_weights}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{save\_model\_weights}}}{}{{ $\rightarrow$ NotImplementedError}}
\end{fulllineitems}


\end{fulllineitems}

\index{ConstAgent (class in agents)@\spxentry{ConstAgent}\spxextra{class in agents}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{MultiAgentMarketRL:agents.ConstAgent}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{agents.}}\sphinxbfcode{\sphinxupquote{ConstAgent}}}{\emph{\DUrole{n}{role}}, \emph{\DUrole{n}{reservation}}, \emph{\DUrole{n}{in\_features}}, \emph{\DUrole{n}{action\_boundary}}, \emph{\DUrole{n}{device}\DUrole{o}{=}\DUrole{default_value}{device(type=\textquotesingle{}cpu\textquotesingle{})}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}~\index{ConstAgent.Optimizer (class in agents)@\spxentry{ConstAgent.Optimizer}\spxextra{class in agents}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{MultiAgentMarketRL:agents.ConstAgent.Optimizer}}\pysigline{\sphinxbfcode{\sphinxupquote{class }}\sphinxbfcode{\sphinxupquote{Optimizer}}}
\sphinxAtStartPar
Dummy optimizer \textendash{}\textgreater{} all members will pass
\index{\_\_init\_\_() (agents.ConstAgent.Optimizer method)@\spxentry{\_\_init\_\_()}\spxextra{agents.ConstAgent.Optimizer method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{MultiAgentMarketRL:agents.ConstAgent.Optimizer.__init__}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{\_\_init\_\_}}}{}{}
\end{fulllineitems}

\index{step() (agents.ConstAgent.Optimizer method)@\spxentry{step()}\spxextra{agents.ConstAgent.Optimizer method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{MultiAgentMarketRL:agents.ConstAgent.Optimizer.step}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{step}}}{}{}
\end{fulllineitems}

\index{zero\_grad() (agents.ConstAgent.Optimizer method)@\spxentry{zero\_grad()}\spxextra{agents.ConstAgent.Optimizer method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{MultiAgentMarketRL:agents.ConstAgent.Optimizer.zero_grad}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{zero\_grad}}}{}{}
\end{fulllineitems}


\end{fulllineitems}

\index{\_\_init\_\_() (agents.ConstAgent method)@\spxentry{\_\_init\_\_()}\spxextra{agents.ConstAgent method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{MultiAgentMarketRL:agents.ConstAgent.__init__}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{\_\_init\_\_}}}{\emph{\DUrole{n}{role}}, \emph{\DUrole{n}{reservation}}, \emph{\DUrole{n}{in\_features}}, \emph{\DUrole{n}{action\_boundary}}, \emph{\DUrole{n}{device}\DUrole{o}{=}\DUrole{default_value}{device(type=\textquotesingle{}cpu\textquotesingle{})}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{{ $\rightarrow$ None}}~\begin{description}
\item[{role: str}] \leavevmode
\sphinxAtStartPar
role can be ‘buyer’ or ‘seller’

\item[{reservation: int}] \leavevmode
\sphinxAtStartPar
The reservation price needs to be in the open interval of (0, infinity). In the case of a seller, the
reservation price denotes the fixed costs and in the case of a buyer, the reservation price denotes the
budget of the agent.

\item[{in\_features: int}] \leavevmode
\sphinxAtStartPar
Number of features observed by the agent

\item[{action\_boundary: int}] \leavevmode
\sphinxAtStartPar
In case the agent is a seller, action\_boundary should equal the largest reservation price of all buyers.
In case the agent is a buyer, action\_boundary should equal the smallest reservation price of all sellers.

\item[{device: torch.device, optional (default=torch.device(‘cpu’))}] \leavevmode
\sphinxAtStartPar
The device on which the agent will run (cpu or gpu)

\item[{kwargs:}] \leavevmode\begin{description}
\item[{const\_price: int (default=(reservation + action\_boundary)//2.0)}] \leavevmode
\sphinxAtStartPar
The constant asking / bidding price

\end{description}

\end{description}

\end{fulllineitems}

\index{get\_action() (agents.ConstAgent method)@\spxentry{get\_action()}\spxextra{agents.ConstAgent method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{MultiAgentMarketRL:agents.ConstAgent.get_action}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{get\_action}}}{\emph{\DUrole{n}{observation}}, \emph{\DUrole{n}{epsilon}\DUrole{o}{=}\DUrole{default_value}{0.05}}}{}
\sphinxAtStartPar
Returns the constant action price of the agent

\sphinxAtStartPar
torch.Tensor of shape (1,) containing the action price of the agent

\end{fulllineitems}

\index{get\_q\_value() (agents.ConstAgent method)@\spxentry{get\_q\_value()}\spxextra{agents.ConstAgent method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{MultiAgentMarketRL:agents.ConstAgent.get_q_value}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{get\_q\_value}}}{\emph{\DUrole{n}{observation}}, \emph{\DUrole{n}{actions}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
\sphinxAtStartPar
Dummy function \textendash{}\textgreater{} ConstAgent is a zero intelligence agent

\end{fulllineitems}

\index{get\_target() (agents.ConstAgent method)@\spxentry{get\_target()}\spxextra{agents.ConstAgent method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{MultiAgentMarketRL:agents.ConstAgent.get_target}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{get\_target}}}{\emph{\DUrole{n}{observation}}, \emph{\DUrole{n}{agent\_state}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
\sphinxAtStartPar
Dummy function \textendash{}\textgreater{} ConstAgent is a zero intelligence agent

\end{fulllineitems}

\index{random\_action() (agents.ConstAgent method)@\spxentry{random\_action()}\spxextra{agents.ConstAgent method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{MultiAgentMarketRL:agents.ConstAgent.random_action}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{random\_action}}}{\emph{\DUrole{n}{observation}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{epsilon}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ NotImplementedError}}
\end{fulllineitems}

\index{reset\_target\_network() (agents.ConstAgent method)@\spxentry{reset\_target\_network()}\spxextra{agents.ConstAgent method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{MultiAgentMarketRL:agents.ConstAgent.reset_target_network}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{reset\_target\_network}}}{}{}
\sphinxAtStartPar
Dummy network reset \textendash{}\textgreater{} will pass

\end{fulllineitems}

\index{save\_model\_weights() (agents.ConstAgent method)@\spxentry{save\_model\_weights()}\spxextra{agents.ConstAgent method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{MultiAgentMarketRL:agents.ConstAgent.save_model_weights}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{save\_model\_weights}}}{}{}
\sphinxAtStartPar
Dummy weight saver \textendash{}\textgreater{} will pass

\end{fulllineitems}


\end{fulllineitems}

\index{DQNAgent (class in agents)@\spxentry{DQNAgent}\spxextra{class in agents}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{MultiAgentMarketRL:agents.DQNAgent}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{agents.}}\sphinxbfcode{\sphinxupquote{DQNAgent}}}{\emph{\DUrole{n}{role}}, \emph{\DUrole{n}{reservation}}, \emph{\DUrole{n}{in\_features}}, \emph{\DUrole{n}{action\_boundary}}, \emph{\DUrole{n}{device}\DUrole{o}{=}\DUrole{default_value}{device(type=\textquotesingle{}cpu\textquotesingle{})}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}~\index{\_\_init\_\_() (agents.DQNAgent method)@\spxentry{\_\_init\_\_()}\spxextra{agents.DQNAgent method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{MultiAgentMarketRL:agents.DQNAgent.__init__}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{\_\_init\_\_}}}{\emph{\DUrole{n}{role}}, \emph{\DUrole{n}{reservation}}, \emph{\DUrole{n}{in\_features}}, \emph{\DUrole{n}{action\_boundary}}, \emph{\DUrole{n}{device}\DUrole{o}{=}\DUrole{default_value}{device(type=\textquotesingle{}cpu\textquotesingle{})}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{{ $\rightarrow$ None}}
\sphinxAtStartPar
Agents are implemented in such a manner, that asking and bidding prices are given as integer values. Therefore
the unit of the price will be equal to the smallest possible unit of currency e.g. for CHF 1 == 0.05 CHF
\begin{description}
\item[{role: str}] \leavevmode
\sphinxAtStartPar
role can be ‘buyer’ or ‘seller’

\item[{reservation: int}] \leavevmode
\sphinxAtStartPar
The reservation price needs to be in the open interval of (0, infinity). In the case of a seller, the
reservation price denotes the fixed costs and in the case of a buyer, the reservation price denotes the
budget of the agent.

\item[{in\_features: int}] \leavevmode
\sphinxAtStartPar
Number of features observed by the agent

\item[{action\_boundary: int}] \leavevmode
\sphinxAtStartPar
In case the agent is a seller, action\_boundary should equal the largest reservation price of all buyers.
In case the agent is a buyer, action\_boundary should equal the smallest reservation price of all sellers.

\item[{device: torch.device, optional (default=torch.device(‘cpu’))}] \leavevmode
\sphinxAtStartPar
The device on which the agent will run (cpu or gpu)

\item[{kwargs:}] \leavevmode\begin{description}
\item[{network\_type: str, optional (default=”SimpleExampleNetwork”)}] \leavevmode
\sphinxAtStartPar
Name of network class implemented in network\_models.py

\item[{q\_lr: float, optional (default=0.001)}] \leavevmode
\sphinxAtStartPar
Learning rate provided to the Q\sphinxhyphen{}Network optimizer

\item[{save\_weights\_directory: str, optional (default=”../saved\_agent\_weights/default\_path/\{self.agent\_name\}/”)}] \leavevmode
\sphinxAtStartPar
Directory to where model weights will be saved to.

\item[{save\_weights\_file: str, optional (default=”default\_test\_file.pt”)}] \leavevmode
\sphinxAtStartPar
File name of the saved weights. Must be a .pt or .pth file

\end{description}

\end{description}

\end{fulllineitems}

\index{get\_action() (agents.DQNAgent method)@\spxentry{get\_action()}\spxextra{agents.DQNAgent method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{MultiAgentMarketRL:agents.DQNAgent.get_action}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{get\_action}}}{\emph{\DUrole{n}{observation}}, \emph{\DUrole{n}{epsilon}\DUrole{o}{=}\DUrole{default_value}{0.05}}}{}
\sphinxAtStartPar
Determines the agent action

\sphinxAtStartPar
observation: torch.Tensor
epsilon: float
\begin{quote}

\sphinxAtStartPar
epsilon defines the exploration rate {[}0,1{]}. With a probability of epsilon the agent will perform a random
action.
\end{quote}

\sphinxAtStartPar
action\_price: int

\end{fulllineitems}

\index{get\_q\_value() (agents.DQNAgent method)@\spxentry{get\_q\_value()}\spxextra{agents.DQNAgent method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{MultiAgentMarketRL:agents.DQNAgent.get_q_value}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{get\_q\_value}}}{\emph{\DUrole{n}{observation}}, \emph{\DUrole{n}{actions}\DUrole{o}{=}\DUrole{default_value}{None}}}{}~\begin{description}
\item[{observation: torch.Tensor}] \leavevmode
\sphinxAtStartPar
Agent observations. Should have shape (batch\_size, observation\_size)

\item[{actions: torch.Tensor, optional (default = False)}] \leavevmode
\sphinxAtStartPar
Will provide the Q values corresponding to the provided actions.
actions should have shape (batch\_size, 1)
If no actions are provided, the Q value will correspond to the maximal value

\end{description}
\begin{description}
\item[{max\_q: torch.Tensor}] \leavevmode
\sphinxAtStartPar
Tensor containing all Q values. Has shape (batch\_size, 1)

\end{description}

\end{fulllineitems}

\index{get\_target() (agents.DQNAgent method)@\spxentry{get\_target()}\spxextra{agents.DQNAgent method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{MultiAgentMarketRL:agents.DQNAgent.get_target}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{get\_target}}}{\emph{\DUrole{n}{observation}}, \emph{\DUrole{n}{agent\_state}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
\sphinxAtStartPar
observation
agent\_state

\end{fulllineitems}

\index{random\_action() (agents.DQNAgent method)@\spxentry{random\_action()}\spxextra{agents.DQNAgent method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{MultiAgentMarketRL:agents.DQNAgent.random_action}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{random\_action}}}{\emph{\DUrole{n}{observation}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{epsilon}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
\sphinxAtStartPar
A uniform random policy intended to populate the replay memory before learning starts
\begin{description}
\item[{observation: None}] \leavevmode
\sphinxAtStartPar
Dummy parameter copying get\_action()

\item[{epsilon: None}] \leavevmode
\sphinxAtStartPar
Dummy parameter copying get\_action()

\end{description}
\begin{description}
\item[{action\_price: int}] \leavevmode
\sphinxAtStartPar
Uniformly sampled action price.

\end{description}

\end{fulllineitems}

\index{reset\_target\_network() (agents.DQNAgent method)@\spxentry{reset\_target\_network()}\spxextra{agents.DQNAgent method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{MultiAgentMarketRL:agents.DQNAgent.reset_target_network}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{reset\_target\_network}}}{}{}
\sphinxAtStartPar
Initialises the target network with the state dictionary of the Q\sphinxhyphen{}network

\end{fulllineitems}

\index{save\_model\_weights() (agents.DQNAgent method)@\spxentry{save\_model\_weights()}\spxextra{agents.DQNAgent method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{MultiAgentMarketRL:agents.DQNAgent.save_model_weights}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{save\_model\_weights}}}{}{}
\sphinxAtStartPar
Saves the model weights in a given directory using a specific file name

\end{fulllineitems}


\end{fulllineitems}

\index{HumanReplayAgent (class in agents)@\spxentry{HumanReplayAgent}\spxextra{class in agents}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{MultiAgentMarketRL:agents.HumanReplayAgent}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{agents.}}\sphinxbfcode{\sphinxupquote{HumanReplayAgent}}}{\emph{\DUrole{n}{role}}, \emph{\DUrole{n}{reservation}}, \emph{\DUrole{n}{in\_features}}, \emph{\DUrole{n}{action\_boundary}}, \emph{\DUrole{n}{device}\DUrole{o}{=}\DUrole{default_value}{device(type=\textquotesingle{}cpu\textquotesingle{})}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}~\index{HumanReplayAgent.Optimizer (class in agents)@\spxentry{HumanReplayAgent.Optimizer}\spxextra{class in agents}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{MultiAgentMarketRL:agents.HumanReplayAgent.Optimizer}}\pysigline{\sphinxbfcode{\sphinxupquote{class }}\sphinxbfcode{\sphinxupquote{Optimizer}}}
\sphinxAtStartPar
Dummy optimizer  \textendash{}\textgreater{} all members will pass
\index{\_\_init\_\_() (agents.HumanReplayAgent.Optimizer method)@\spxentry{\_\_init\_\_()}\spxextra{agents.HumanReplayAgent.Optimizer method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{MultiAgentMarketRL:agents.HumanReplayAgent.Optimizer.__init__}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{\_\_init\_\_}}}{}{}
\end{fulllineitems}

\index{step() (agents.HumanReplayAgent.Optimizer method)@\spxentry{step()}\spxextra{agents.HumanReplayAgent.Optimizer method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{MultiAgentMarketRL:agents.HumanReplayAgent.Optimizer.step}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{step}}}{}{}
\end{fulllineitems}

\index{zero\_grad() (agents.HumanReplayAgent.Optimizer method)@\spxentry{zero\_grad()}\spxextra{agents.HumanReplayAgent.Optimizer method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{MultiAgentMarketRL:agents.HumanReplayAgent.Optimizer.zero_grad}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{zero\_grad}}}{}{}
\end{fulllineitems}


\end{fulllineitems}

\index{\_\_init\_\_() (agents.HumanReplayAgent method)@\spxentry{\_\_init\_\_()}\spxextra{agents.HumanReplayAgent method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{MultiAgentMarketRL:agents.HumanReplayAgent.__init__}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{\_\_init\_\_}}}{\emph{\DUrole{n}{role}}, \emph{\DUrole{n}{reservation}}, \emph{\DUrole{n}{in\_features}}, \emph{\DUrole{n}{action\_boundary}}, \emph{\DUrole{n}{device}\DUrole{o}{=}\DUrole{default_value}{device(type=\textquotesingle{}cpu\textquotesingle{})}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{{ $\rightarrow$ None}}~\begin{description}
\item[{role: str}] \leavevmode
\sphinxAtStartPar
role can be ‘buyer’ or ‘seller’

\item[{reservation: int}] \leavevmode
\sphinxAtStartPar
The reservation price needs to be in the open interval of (0, infinity). In the case of a seller, the
reservation price denotes the fixed costs and in the case of a buyer, the reservation price denotes the
budget of the agent.

\item[{in\_features: int}] \leavevmode
\sphinxAtStartPar
Number of features observed by the agent

\item[{action\_boundary: int}] \leavevmode
\sphinxAtStartPar
In case the agent is a seller, action\_boundary should equal the largest reservation price of all buyers.
In case the agent is a buyer, action\_boundary should equal the smallest reservation price of all sellers.

\item[{device: torch.device, optional (default=torch.device(‘cpu’))}] \leavevmode
\sphinxAtStartPar
The device on which the agent will run (cpu or gpu)

\item[{kwargs:}] \leavevmode\begin{description}
\item[{data\_type: str, optional (default=’new\_data’)}] \leavevmode
\sphinxAtStartPar
Data set used (new\_data or old\_data). See the git directory ‘HumanReplayData’

\item[{treatment: str, optional (default=’FullLimS’)}] \leavevmode
\sphinxAtStartPar
Market treatment used. See \sphinxurl{https://papers.ssrn.com/sol3/papers.cfm?abstract\_id=3131004}

\item[{id: int, optional (default=954)}] \leavevmode
\sphinxAtStartPar
Player id. Must match with agent ‘role’, ‘reservation’, ‘data\_type’ and ‘treatment’.
See the .csv files in the git directory ‘HumanReplayData/data\_type’

\end{description}

\end{description}

\end{fulllineitems}

\index{get\_action() (agents.HumanReplayAgent method)@\spxentry{get\_action()}\spxextra{agents.HumanReplayAgent method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{MultiAgentMarketRL:agents.HumanReplayAgent.get_action}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{get\_action}}}{\emph{\DUrole{n}{observation}}, \emph{\DUrole{n}{epsilon}\DUrole{o}{=}\DUrole{default_value}{0.05}}}{}
\sphinxAtStartPar
Returns the next bid from the data set for every market step. If all data steps have been used, we will restart
from the beginning.

\sphinxAtStartPar
torch.Tensor of shape (1,) containing the bid price of the agent

\end{fulllineitems}

\index{get\_q\_value() (agents.HumanReplayAgent method)@\spxentry{get\_q\_value()}\spxextra{agents.HumanReplayAgent method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{MultiAgentMarketRL:agents.HumanReplayAgent.get_q_value}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{get\_q\_value}}}{\emph{\DUrole{n}{observation}}, \emph{\DUrole{n}{actions}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
\sphinxAtStartPar
Dummy function \textendash{}\textgreater{} HumanReplayAgent is a zero intelligence agent

\end{fulllineitems}

\index{get\_target() (agents.HumanReplayAgent method)@\spxentry{get\_target()}\spxextra{agents.HumanReplayAgent method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{MultiAgentMarketRL:agents.HumanReplayAgent.get_target}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{get\_target}}}{\emph{\DUrole{n}{observation}}, \emph{\DUrole{n}{agent\_state}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
\sphinxAtStartPar
Dummy function \textendash{}\textgreater{} HumanReplayAgent is a zero intelligence agent

\end{fulllineitems}

\index{random\_action() (agents.HumanReplayAgent method)@\spxentry{random\_action()}\spxextra{agents.HumanReplayAgent method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{MultiAgentMarketRL:agents.HumanReplayAgent.random_action}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{random\_action}}}{\emph{\DUrole{n}{observation}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{epsilon}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ NotImplementedError}}
\end{fulllineitems}

\index{reset\_target\_network() (agents.HumanReplayAgent method)@\spxentry{reset\_target\_network()}\spxextra{agents.HumanReplayAgent method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{MultiAgentMarketRL:agents.HumanReplayAgent.reset_target_network}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{reset\_target\_network}}}{}{}
\sphinxAtStartPar
Dummy network reset \textendash{}\textgreater{} will pass

\end{fulllineitems}

\index{save\_model\_weights() (agents.HumanReplayAgent method)@\spxentry{save\_model\_weights()}\spxextra{agents.HumanReplayAgent method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{MultiAgentMarketRL:agents.HumanReplayAgent.save_model_weights}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{save\_model\_weights}}}{}{}
\sphinxAtStartPar
Dummy weight saver \textendash{}\textgreater{} will pass

\end{fulllineitems}


\end{fulllineitems}

\phantomsection\label{\detokenize{MultiAgentMarketRL:module-network_models}}\index{module@\spxentry{module}!network\_models@\spxentry{network\_models}}\index{network\_models@\spxentry{network\_models}!module@\spxentry{module}}\index{NetworkSetting (class in network\_models)@\spxentry{NetworkSetting}\spxextra{class in network\_models}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{MultiAgentMarketRL:network_models.NetworkSetting}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{network\_models.}}\sphinxbfcode{\sphinxupquote{NetworkSetting}}}{\emph{\DUrole{n}{in\_features}}, \emph{\DUrole{n}{out\_features}}, \emph{\DUrole{n}{device}\DUrole{o}{=}\DUrole{default_value}{device(type=\textquotesingle{}cpu\textquotesingle{})}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\sphinxAtStartPar
Abstract network setting class.
Used to define your custom neural networks which can then be used by intelligent agents
\index{\_\_init\_\_() (network\_models.NetworkSetting method)@\spxentry{\_\_init\_\_()}\spxextra{network\_models.NetworkSetting method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{MultiAgentMarketRL:network_models.NetworkSetting.__init__}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{\_\_init\_\_}}}{\emph{\DUrole{n}{in\_features}}, \emph{\DUrole{n}{out\_features}}, \emph{\DUrole{n}{device}\DUrole{o}{=}\DUrole{default_value}{device(type=\textquotesingle{}cpu\textquotesingle{})}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\sphinxAtStartPar
Initialises in\_features and out\_features needed to construct a fitting neural network
\begin{description}
\item[{in\_features: int}] \leavevmode
\sphinxAtStartPar
Determined by the info\_setting \textendash{}\textgreater{} How many features does the agent see

\item[{out\_features: int}] \leavevmode
\sphinxAtStartPar
Determined by the action space \textendash{}\textgreater{} Number of legal actions including No action

\item[{device: torch.device, optional (default cpu)}] \leavevmode
\sphinxAtStartPar
Device on which the neural network is intended to run (cpu or gpu)

\item[{kwargs: Optional additional keyword arguments}] \leavevmode\begin{description}
\item[{load\_weights\_path: str, optional (default=False)}] \leavevmode
\sphinxAtStartPar
If a path is provided, agent will try to load pretrained weights from there.

\end{description}

\end{description}

\end{fulllineitems}

\index{define\_network() (network\_models.NetworkSetting method)@\spxentry{define\_network()}\spxextra{network\_models.NetworkSetting method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{MultiAgentMarketRL:network_models.NetworkSetting.define_network}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{define\_network}}}{}{}~\begin{description}
\item[{network: torch.nn}] \leavevmode
\sphinxAtStartPar
The wanted neural network

\end{description}

\end{fulllineitems}

\index{get\_network() (network\_models.NetworkSetting method)@\spxentry{get\_network()}\spxextra{network\_models.NetworkSetting method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{MultiAgentMarketRL:network_models.NetworkSetting.get_network}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{get\_network}}}{}{}
\sphinxAtStartPar
Will return the wanted neural network model located on the intended device (cpu or gpu)
\begin{description}
\item[{network: torch.nn}] \leavevmode
\sphinxAtStartPar
The wanted neural network on the correct device

\end{description}

\end{fulllineitems}


\end{fulllineitems}

\index{SimpleExampleNetwork (class in network\_models)@\spxentry{SimpleExampleNetwork}\spxextra{class in network\_models}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{MultiAgentMarketRL:network_models.SimpleExampleNetwork}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{network\_models.}}\sphinxbfcode{\sphinxupquote{SimpleExampleNetwork}}}{\emph{\DUrole{n}{in\_features}}, \emph{\DUrole{n}{out\_features}}, \emph{\DUrole{n}{device}\DUrole{o}{=}\DUrole{default_value}{device(type=\textquotesingle{}cpu\textquotesingle{})}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\sphinxAtStartPar
A simple network fulfilling the role of being an example
\index{\_\_init\_\_() (network\_models.SimpleExampleNetwork method)@\spxentry{\_\_init\_\_()}\spxextra{network\_models.SimpleExampleNetwork method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{MultiAgentMarketRL:network_models.SimpleExampleNetwork.__init__}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{\_\_init\_\_}}}{\emph{\DUrole{n}{in\_features}}, \emph{\DUrole{n}{out\_features}}, \emph{\DUrole{n}{device}\DUrole{o}{=}\DUrole{default_value}{device(type=\textquotesingle{}cpu\textquotesingle{})}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\sphinxAtStartPar
Initialises in\_features and out\_features needed to construct a fitting neural network
\begin{description}
\item[{in\_features: int}] \leavevmode
\sphinxAtStartPar
Determined by the info\_setting \textendash{}\textgreater{} How many features does the agent see

\item[{out\_features: int}] \leavevmode
\sphinxAtStartPar
Determined by the action space \textendash{}\textgreater{} Number of legal actions including No action

\item[{device: torch.device, optional (default cpu)}] \leavevmode
\sphinxAtStartPar
Device on which the neural network is intended to run (cpu or gpu)

\item[{kwargs: Optional additional keyword arguments}] \leavevmode\begin{description}
\item[{load\_weights\_path: str, optional (default=False)}] \leavevmode
\sphinxAtStartPar
If a path is provided, agent will try to load pretrained weights from there.

\end{description}

\end{description}

\end{fulllineitems}

\index{define\_network() (network\_models.SimpleExampleNetwork method)@\spxentry{define\_network()}\spxextra{network\_models.SimpleExampleNetwork method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{MultiAgentMarketRL:network_models.SimpleExampleNetwork.define_network}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{define\_network}}}{}{}
\sphinxAtStartPar
Defines a simple network for the purpose of being an example
\begin{description}
\item[{network: torch.nn}] \leavevmode
\sphinxAtStartPar
The wanted neural network

\end{description}

\end{fulllineitems}

\index{get\_network() (network\_models.SimpleExampleNetwork method)@\spxentry{get\_network()}\spxextra{network\_models.SimpleExampleNetwork method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{MultiAgentMarketRL:network_models.SimpleExampleNetwork.get_network}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{get\_network}}}{}{}
\sphinxAtStartPar
Will return the wanted neural network model located on the intended device (cpu or gpu)
\begin{description}
\item[{network: torch.nn}] \leavevmode
\sphinxAtStartPar
The wanted neural network on the correct device

\end{description}

\end{fulllineitems}


\end{fulllineitems}

\phantomsection\label{\detokenize{MultiAgentMarketRL:module-info_setting}}\index{module@\spxentry{module}!info\_setting@\spxentry{info\_setting}}\index{info\_setting@\spxentry{info\_setting}!module@\spxentry{module}}\index{BlackBoxSetting (class in info\_setting)@\spxentry{BlackBoxSetting}\spxextra{class in info\_setting}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{MultiAgentMarketRL:info_setting.BlackBoxSetting}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{info\_setting.}}\sphinxbfcode{\sphinxupquote{BlackBoxSetting}}}{\emph{\DUrole{n}{env}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\sphinxAtStartPar
The agent is aware of only its own last offer.
\index{\_\_init\_\_() (info\_setting.BlackBoxSetting method)@\spxentry{\_\_init\_\_()}\spxextra{info\_setting.BlackBoxSetting method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{MultiAgentMarketRL:info_setting.BlackBoxSetting.__init__}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{\_\_init\_\_}}}{\emph{\DUrole{n}{env}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\sphinxAtStartPar
env: environment class object
kwargs:
\begin{quote}

\sphinxAtStartPar
Additional keyword arguments
\end{quote}

\end{fulllineitems}

\index{get\_states() (info\_setting.BlackBoxSetting method)@\spxentry{get\_states()}\spxextra{info\_setting.BlackBoxSetting method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{MultiAgentMarketRL:info_setting.BlackBoxSetting.get_states}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{get\_states}}}{}{}~\begin{description}
\item[{total\_info: torch.tensor}] \leavevmode
\sphinxAtStartPar
Return total\_info as tensor with shape (n\_agents, n\_features) where n\_features == 1
Observations are ordered in the same way as res in MultiAgentEnvironment.get\_actions().
total\_info{[}:n\_sellers, :{]} contains all observations for the seller agents
total\_info{[}n\_sellers:, :{]} contains all observations for the buyer agents

\end{description}

\end{fulllineitems}


\end{fulllineitems}

\index{DealInformationSetting (class in info\_setting)@\spxentry{DealInformationSetting}\spxextra{class in info\_setting}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{MultiAgentMarketRL:info_setting.DealInformationSetting}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{info\_setting.}}\sphinxbfcode{\sphinxupquote{DealInformationSetting}}}{\emph{\DUrole{n}{env}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\sphinxAtStartPar
The agent is aware of N deals of the last round.

\sphinxAtStartPar
Note: the N deals need not be sorted on deal price. It depends the order
the matcher matches deals, see \sphinxcode{\sphinxupquote{MarketEngine.matcher}}.
\begin{description}
\item[{kwargs: dict}] \leavevmode\begin{description}
\item[{n\_deals: int, optional (default=1)}] \leavevmode
\sphinxAtStartPar
Number of deals to see.

\end{description}

\end{description}
\index{\_\_init\_\_() (info\_setting.DealInformationSetting method)@\spxentry{\_\_init\_\_()}\spxextra{info\_setting.DealInformationSetting method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{MultiAgentMarketRL:info_setting.DealInformationSetting.__init__}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{\_\_init\_\_}}}{\emph{\DUrole{n}{env}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\sphinxAtStartPar
env: environment class object
kwargs:
\begin{quote}

\sphinxAtStartPar
‘n\_deals’ (int)
\end{quote}

\end{fulllineitems}

\index{get\_states() (info\_setting.DealInformationSetting method)@\spxentry{get\_states()}\spxextra{info\_setting.DealInformationSetting method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{MultiAgentMarketRL:info_setting.DealInformationSetting.get_states}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{get\_states}}}{}{}~\begin{description}
\item[{total\_info: torch.tensor}] \leavevmode
\sphinxAtStartPar
Return total\_info as tensor with shape (n\_agents, n\_features) where n\_features == n\_deals
Observations are ordered in the same way as res in MultiAgentEnvironment.get\_actions().
total\_info{[}:n\_sellers, :{]} contains all observations for the seller agents
total\_info{[}n\_sellers:, :{]} contains all observations for the buyer agents

\end{description}

\end{fulllineitems}


\end{fulllineitems}

\index{InformationSetting (class in info\_setting)@\spxentry{InformationSetting}\spxextra{class in info\_setting}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{MultiAgentMarketRL:info_setting.InformationSetting}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{info\_setting.}}\sphinxbfcode{\sphinxupquote{InformationSetting}}}{\emph{\DUrole{n}{env}}}{}
\sphinxAtStartPar
Abstract information setting class.
\begin{description}
\item[{env: Environment object}] \leavevmode
\sphinxAtStartPar
The current environment class object.

\end{description}
\index{\_\_init\_\_() (info\_setting.InformationSetting method)@\spxentry{\_\_init\_\_()}\spxextra{info\_setting.InformationSetting method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{MultiAgentMarketRL:info_setting.InformationSetting.__init__}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{\_\_init\_\_}}}{\emph{\DUrole{n}{env}}}{}
\sphinxAtStartPar
env: environment class object

\end{fulllineitems}

\index{get\_states() (info\_setting.InformationSetting method)@\spxentry{get\_states()}\spxextra{info\_setting.InformationSetting method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{MultiAgentMarketRL:info_setting.InformationSetting.get_states}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{get\_states}}}{}{}
\sphinxAtStartPar
Compute the observations of agents given the market object.

\end{fulllineitems}


\end{fulllineitems}

\index{OfferInformationSetting (class in info\_setting)@\spxentry{OfferInformationSetting}\spxextra{class in info\_setting}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{MultiAgentMarketRL:info_setting.OfferInformationSetting}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{info\_setting.}}\sphinxbfcode{\sphinxupquote{OfferInformationSetting}}}{\emph{\DUrole{n}{env}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\sphinxAtStartPar
The agent is aware of the best N offers of either side of the last round.
\begin{description}
\item[{kwargs: dict}] \leavevmode\begin{description}
\item[{n\_offers: int, optional (default=1)}] \leavevmode
\sphinxAtStartPar
Number of offers to see. For instance, 5 would mean the agents see the
best 5 bids and asks.

\end{description}

\end{description}
\index{\_\_init\_\_() (info\_setting.OfferInformationSetting method)@\spxentry{\_\_init\_\_()}\spxextra{info\_setting.OfferInformationSetting method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{MultiAgentMarketRL:info_setting.OfferInformationSetting.__init__}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{\_\_init\_\_}}}{\emph{\DUrole{n}{env}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\sphinxAtStartPar
env: environment class object
kwargs:
\begin{quote}

\sphinxAtStartPar
‘n\_offers’ (int)
\end{quote}

\end{fulllineitems}

\index{get\_states() (info\_setting.OfferInformationSetting method)@\spxentry{get\_states()}\spxextra{info\_setting.OfferInformationSetting method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{MultiAgentMarketRL:info_setting.OfferInformationSetting.get_states}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{get\_states}}}{}{}~\begin{description}
\item[{total\_info: torch.tensor}] \leavevmode
\sphinxAtStartPar
Return total\_info as tensor with shape (n\_agents, n\_features) where n\_features == 2*n\_offers
Observations are ordered in the same way as res in MultiAgentEnvironment.get\_actions().
total\_info{[}:n\_sellers, :{]} contains all observations for the seller agents
total\_info{[}n\_sellers:, :{]} contains all observations for the buyer agents

\end{description}

\end{fulllineitems}


\end{fulllineitems}

\index{TimeInformationWrapper (class in info\_setting)@\spxentry{TimeInformationWrapper}\spxextra{class in info\_setting}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{MultiAgentMarketRL:info_setting.TimeInformationWrapper}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{info\_setting.}}\sphinxbfcode{\sphinxupquote{TimeInformationWrapper}}}{\emph{\DUrole{n}{env}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\sphinxAtStartPar
Wrapper to include the time in the observation.

\sphinxAtStartPar
This class takes as input another information setting and adds the time
of the market to the observations of that information setting. This allows
certain fixed agents to adopt time\sphinxhyphen{}dependent strategies.
\begin{description}
\item[{kwargs: dict}] \leavevmode\begin{description}
\item[{base\_setting: InformationSetting object}] \leavevmode
\sphinxAtStartPar
The base information setting to add time to.

\end{description}

\end{description}
\index{\_\_init\_\_() (info\_setting.TimeInformationWrapper method)@\spxentry{\_\_init\_\_()}\spxextra{info\_setting.TimeInformationWrapper method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{MultiAgentMarketRL:info_setting.TimeInformationWrapper.__init__}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{\_\_init\_\_}}}{\emph{\DUrole{n}{env}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\sphinxAtStartPar
env: environment class object
kwargs:
\begin{quote}

\sphinxAtStartPar
‘base\_setting’ (str)
\end{quote}

\end{fulllineitems}

\index{get\_states() (info\_setting.TimeInformationWrapper method)@\spxentry{get\_states()}\spxextra{info\_setting.TimeInformationWrapper method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{MultiAgentMarketRL:info_setting.TimeInformationWrapper.get_states}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{get\_states}}}{}{}~\begin{description}
\item[{total\_info: torch.tensor}] \leavevmode
\sphinxAtStartPar
Return total\_info as tensor with shape (n\_agents, n\_features) where n\_features == base\_features + 1
Observations are ordered in the same way as res in MultiAgentEnvironment.get\_actions().
total\_info{[}:n\_sellers, :{]} contains all observations for the seller agents
total\_info{[}n\_sellers:, :{]} contains all observations for the buyer agents

\end{description}

\end{fulllineitems}


\end{fulllineitems}

\phantomsection\label{\detokenize{MultiAgentMarketRL:module-exploration_setting}}\index{module@\spxentry{module}!exploration\_setting@\spxentry{exploration\_setting}}\index{exploration\_setting@\spxentry{exploration\_setting}!module@\spxentry{module}}\index{ExplorationSetting (class in exploration\_setting)@\spxentry{ExplorationSetting}\spxextra{class in exploration\_setting}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{MultiAgentMarketRL:exploration_setting.ExplorationSetting}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{exploration\_setting.}}\sphinxbfcode{\sphinxupquote{ExplorationSetting}}}{\emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\sphinxAtStartPar
Abstract exploration setting class
\index{\_\_init\_\_() (exploration\_setting.ExplorationSetting method)@\spxentry{\_\_init\_\_()}\spxextra{exploration\_setting.ExplorationSetting method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{MultiAgentMarketRL:exploration_setting.ExplorationSetting.__init__}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{\_\_init\_\_}}}{\emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\end{fulllineitems}

\index{update() (exploration\_setting.ExplorationSetting method)@\spxentry{update()}\spxextra{exploration\_setting.ExplorationSetting method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{MultiAgentMarketRL:exploration_setting.ExplorationSetting.update}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{update}}}{}{}
\end{fulllineitems}


\end{fulllineitems}

\index{LinearExplorationDecline (class in exploration\_setting)@\spxentry{LinearExplorationDecline}\spxextra{class in exploration\_setting}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{MultiAgentMarketRL:exploration_setting.LinearExplorationDecline}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{exploration\_setting.}}\sphinxbfcode{\sphinxupquote{LinearExplorationDecline}}}{\emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}~\begin{description}
\item[{kwargs:}] \leavevmode\begin{description}
\item[{initial\_expo: float, optional (default=1.0)}] \leavevmode
\sphinxAtStartPar
Initial exploration probability

\item[{n\_expo\_steps: int, optional (default=100000)}] \leavevmode
\sphinxAtStartPar
Number of time steps over which the exploration rate will decrease linearly

\item[{final\_expo: float, optional (default=0.0)}] \leavevmode
\sphinxAtStartPar
Final exploration rate

\end{description}

\end{description}
\index{\_\_init\_\_() (exploration\_setting.LinearExplorationDecline method)@\spxentry{\_\_init\_\_()}\spxextra{exploration\_setting.LinearExplorationDecline method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{MultiAgentMarketRL:exploration_setting.LinearExplorationDecline.__init__}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{\_\_init\_\_}}}{\emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\end{fulllineitems}

\index{reset() (exploration\_setting.LinearExplorationDecline method)@\spxentry{reset()}\spxextra{exploration\_setting.LinearExplorationDecline method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{MultiAgentMarketRL:exploration_setting.LinearExplorationDecline.reset}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{reset}}}{}{}
\end{fulllineitems}

\index{update() (exploration\_setting.LinearExplorationDecline method)@\spxentry{update()}\spxextra{exploration\_setting.LinearExplorationDecline method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{MultiAgentMarketRL:exploration_setting.LinearExplorationDecline.update}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{update}}}{}{}
\end{fulllineitems}


\end{fulllineitems}

\phantomsection\label{\detokenize{MultiAgentMarketRL:module-reward_setting}}\index{module@\spxentry{module}!reward\_setting@\spxentry{reward\_setting}}\index{reward\_setting@\spxentry{reward\_setting}!module@\spxentry{module}}\index{NoDealPenaltyReward (class in reward\_setting)@\spxentry{NoDealPenaltyReward}\spxextra{class in reward\_setting}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{MultiAgentMarketRL:reward_setting.NoDealPenaltyReward}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{reward\_setting.}}\sphinxbfcode{\sphinxupquote{NoDealPenaltyReward}}}{\emph{\DUrole{n}{env}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\sphinxAtStartPar
Punishes buyers with a linearly increasing negative reward if they do not manage to close a deal after N rounds.
\begin{description}
\item[{kwargs:}] \leavevmode\begin{description}
\item[{no\_deal\_max: int, optional (default=10)}] \leavevmode
\sphinxAtStartPar
Number of allowed time steps without making a deal before being punished

\end{description}

\end{description}
\index{\_\_init\_\_() (reward\_setting.NoDealPenaltyReward method)@\spxentry{\_\_init\_\_()}\spxextra{reward\_setting.NoDealPenaltyReward method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{MultiAgentMarketRL:reward_setting.NoDealPenaltyReward.__init__}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{\_\_init\_\_}}}{\emph{\DUrole{n}{env}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\end{fulllineitems}

\index{buyer\_reward() (reward\_setting.NoDealPenaltyReward method)@\spxentry{buyer\_reward()}\spxextra{reward\_setting.NoDealPenaltyReward method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{MultiAgentMarketRL:reward_setting.NoDealPenaltyReward.buyer_reward}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{buyer\_reward}}}{\emph{\DUrole{n}{buyer\_deals}}}{}~\begin{description}
\item[{buyer\_deals: torch.tensor}] \leavevmode
\sphinxAtStartPar
Has shape (n\_buyers)

\end{description}
\begin{description}
\item[{rew: torch.tensor}] \leavevmode
\sphinxAtStartPar
Has shape (n\_buyers)

\end{description}

\end{fulllineitems}

\index{seller\_reward() (reward\_setting.NoDealPenaltyReward method)@\spxentry{seller\_reward()}\spxextra{reward\_setting.NoDealPenaltyReward method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{MultiAgentMarketRL:reward_setting.NoDealPenaltyReward.seller_reward}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{seller\_reward}}}{\emph{\DUrole{n}{seller\_deals}}}{}~\begin{description}
\item[{seller\_deals: torch.tensor}] \leavevmode
\sphinxAtStartPar
Has shape (n\_sellers,)

\end{description}
\begin{description}
\item[{rew: torch.tensor}] \leavevmode
\sphinxAtStartPar
Has shape (n\_sellers)

\end{description}

\end{fulllineitems}


\end{fulllineitems}

\index{RewardSetting (class in reward\_setting)@\spxentry{RewardSetting}\spxextra{class in reward\_setting}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{MultiAgentMarketRL:reward_setting.RewardSetting}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{reward\_setting.}}\sphinxbfcode{\sphinxupquote{RewardSetting}}}{\emph{\DUrole{n}{env}}}{}
\sphinxAtStartPar
Abstract reward setting class
\index{\_\_init\_\_() (reward\_setting.RewardSetting method)@\spxentry{\_\_init\_\_()}\spxextra{reward\_setting.RewardSetting method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{MultiAgentMarketRL:reward_setting.RewardSetting.__init__}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{\_\_init\_\_}}}{\emph{\DUrole{n}{env}}}{}
\end{fulllineitems}

\index{buyer\_reward() (reward\_setting.RewardSetting method)@\spxentry{buyer\_reward()}\spxextra{reward\_setting.RewardSetting method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{MultiAgentMarketRL:reward_setting.RewardSetting.buyer_reward}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{buyer\_reward}}}{\emph{\DUrole{n}{buyer\_deals}}}{}
\end{fulllineitems}

\index{seller\_reward() (reward\_setting.RewardSetting method)@\spxentry{seller\_reward()}\spxextra{reward\_setting.RewardSetting method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{MultiAgentMarketRL:reward_setting.RewardSetting.seller_reward}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{seller\_reward}}}{\emph{\DUrole{n}{seller\_deals}}}{}
\end{fulllineitems}


\end{fulllineitems}

\phantomsection\label{\detokenize{MultiAgentMarketRL:module-environment}}\index{module@\spxentry{module}!environment@\spxentry{environment}}\index{environment@\spxentry{environment}!module@\spxentry{module}}\index{MultiAgentEnvironment (class in environment)@\spxentry{MultiAgentEnvironment}\spxextra{class in environment}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{MultiAgentMarketRL:environment.MultiAgentEnvironment}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{environment.}}\sphinxbfcode{\sphinxupquote{MultiAgentEnvironment}}}{\emph{\DUrole{n}{agent\_dict}}, \emph{\DUrole{n}{market}}, \emph{\DUrole{n}{info\_setting}}, \emph{\DUrole{n}{exploration\_setting}}, \emph{\DUrole{n}{reward\_setting}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}~\index{\_\_init\_\_() (environment.MultiAgentEnvironment method)@\spxentry{\_\_init\_\_()}\spxextra{environment.MultiAgentEnvironment method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{MultiAgentMarketRL:environment.MultiAgentEnvironment.__init__}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{\_\_init\_\_}}}{\emph{\DUrole{n}{agent\_dict}}, \emph{\DUrole{n}{market}}, \emph{\DUrole{n}{info\_setting}}, \emph{\DUrole{n}{exploration\_setting}}, \emph{\DUrole{n}{reward\_setting}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}~\begin{description}
\item[{agent\_dict: dict}] \leavevmode\begin{description}
\item[{Nested dictionary of shape \{}] \leavevmode\begin{description}
\item[{‘sellers’: \{1: \{}] \leavevmode\begin{quote}

\sphinxAtStartPar
‘type’: ‘DQNAgent’,
‘reservation’: 12,
‘multiplicity’: 3,
{\color{red}\bfseries{}**}kwargs
\}
\end{quote}

\sphinxAtStartPar
…
\}

\item[{‘buyers’: \{1: \{}] \leavevmode\begin{quote}

\sphinxAtStartPar
‘type’: ‘DQNAgent’,
‘reservation’: 5,
{\color{red}\bfseries{}**}kwargs
\}
\end{quote}

\sphinxAtStartPar
…
\}

\end{description}

\sphinxAtStartPar
\}

\item[{Containing meta\_info for all wanted sellers and buyers}] \leavevmode\begin{description}
\item[{Mandatory keywords are:}] \leavevmode\begin{description}
\item[{‘type’: str}] \leavevmode
\sphinxAtStartPar
Name of the wanted agents class object

\item[{‘reservation’: int}] \leavevmode
\sphinxAtStartPar
Reservation price for this agent

\end{description}

\item[{Optional keywords are:}] \leavevmode\begin{description}
\item[{‘multiplicity’: int}] \leavevmode
\sphinxAtStartPar
Multiplicity count of the agent (default=1)

\item[{{\color{red}\bfseries{}**}kwargs:}] \leavevmode
\sphinxAtStartPar
Additional keyword arguments specific to the agent type

\end{description}

\end{description}

\end{description}

\end{description}

\sphinxAtStartPar
market: str or markets class object
info\_setting: str or info\_setting class object
exploration\_setting: str or exploration\_setting class object
reward\_setting: str or reward\_setting class object
kwargs: dict, optional
\begin{quote}
\begin{description}
\item[{Nested dictionary of shape \{}] \leavevmode
\sphinxAtStartPar
‘market\_settings’: \{….\},
‘info\_settings’: \{….\},
‘exploration\_settings’: \{….\},
‘reward\_settings’: \{….\},
‘devices’:…
\}

\end{description}

\sphinxAtStartPar
Allowing to fine tune keyword arguments of the individual settings.
‘device’ is responsible for providing GPU support.
The environment is thought to run on two GPUs. One GPU for the environment and one for the
agent optimization. If provided should be a list of two GPU devices.
First device will be for the environment, second device will be for agent networks.
Default is CPU.
\end{quote}

\end{fulllineitems}

\index{calculate\_rewards() (environment.MultiAgentEnvironment method)@\spxentry{calculate\_rewards()}\spxextra{environment.MultiAgentEnvironment method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{MultiAgentMarketRL:environment.MultiAgentEnvironment.calculate_rewards}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{calculate\_rewards}}}{\emph{\DUrole{n}{deals\_sellers}}, \emph{\DUrole{n}{deals\_buyers}}}{}
\end{fulllineitems}

\index{get\_actions() (environment.MultiAgentEnvironment method)@\spxentry{get\_actions()}\spxextra{environment.MultiAgentEnvironment method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{MultiAgentMarketRL:environment.MultiAgentEnvironment.get_actions}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{get\_actions}}}{}{}
\end{fulllineitems}

\index{reset() (environment.MultiAgentEnvironment method)@\spxentry{reset()}\spxextra{environment.MultiAgentEnvironment method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{MultiAgentMarketRL:environment.MultiAgentEnvironment.reset}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{reset}}}{}{}
\end{fulllineitems}

\index{step() (environment.MultiAgentEnvironment method)@\spxentry{step()}\spxextra{environment.MultiAgentEnvironment method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{MultiAgentMarketRL:environment.MultiAgentEnvironment.step}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{step}}}{\emph{\DUrole{n}{random\_action}\DUrole{o}{=}\DUrole{default_value}{False}}}{}
\sphinxAtStartPar
random\_action
\begin{description}
\item[{current\_observations: torch.Tensor}] \leavevmode
\sphinxAtStartPar
All agent observations at the current time step t. The zero dimension has size self.n\_agents
current\_observations{[}:n\_sellers, :{]} contains all observations for the seller agents
current\_observations{[}n\_sellers:, :{]} contains all observations for the buyer agents

\item[{current\_actions: torch.Tensor}] \leavevmode
\sphinxAtStartPar
All agent actions at the current time step t. The last dimension has size self.n\_agents
current\_actions{[}:n\_sellers{]} contains all actions for the seller agents
current\_actions{[}n\_sellers:{]} contains all actions for the buyer agents

\item[{current\_rewards: torch.Tensor}] \leavevmode
\sphinxAtStartPar
All agent rewards at the current time step t. The last dimension has size self.n\_agents
current\_rewards{[}:n\_sellers{]} contains all rewards for the seller agents
current\_rewards{[}n\_sellers:{]} contains all rewards for the buyer agents

\item[{next\_observation: torch.Tensor}] \leavevmode
\sphinxAtStartPar
All agent observations at the next time step t + 1. The zero dimension has size self.n\_agents
next\_observation{[}:n\_sellers, :{]} contains all observations for the seller agents
next\_observation{[}n\_sellers:, :{]} contains all observations for the buyer agents

\item[{self.done: bool}] \leavevmode
\sphinxAtStartPar
True if all agents are done trading or if the the game has come to an end

\end{description}

\end{fulllineitems}

\index{store\_observations() (environment.MultiAgentEnvironment method)@\spxentry{store\_observations()}\spxextra{environment.MultiAgentEnvironment method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{MultiAgentMarketRL:environment.MultiAgentEnvironment.store_observations}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{store\_observations}}}{}{}
\end{fulllineitems}


\end{fulllineitems}

\index{generate\_agents() (in module environment)@\spxentry{generate\_agents()}\spxextra{in module environment}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{MultiAgentMarketRL:environment.generate_agents}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{environment.}}\sphinxbfcode{\sphinxupquote{generate\_agents}}}{\emph{\DUrole{n}{agent\_dict}}, \emph{\DUrole{n}{device}}}{}
\end{fulllineitems}

\index{get\_agent\_actions() (in module environment)@\spxentry{get\_agent\_actions()}\spxextra{in module environment}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{MultiAgentMarketRL:environment.get_agent_actions}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{environment.}}\sphinxbfcode{\sphinxupquote{get\_agent\_actions}}}{\emph{\DUrole{n}{agent}}, \emph{\DUrole{n}{observation}}, \emph{\DUrole{n}{epsilon}}, \emph{\DUrole{n}{random\_action}}}{}
\sphinxAtStartPar
agent: Abstract agent class
observation: torch.Tensor
epsilon: float
\begin{quote}

\sphinxAtStartPar
The probability of returning a random action in epsilon\sphinxhyphen{}greedy exploration
\end{quote}
\begin{description}
\item[{random\_action: bool}] \leavevmode
\sphinxAtStartPar
If true action is drawn from a uniform random policy or from a specifically implemented random policy called
‘random\_action’

\end{description}

\sphinxAtStartPar
action: torch.Tensor

\end{fulllineitems}

\index{get\_basic\_agent\_info() (in module environment)@\spxentry{get\_basic\_agent\_info()}\spxextra{in module environment}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{MultiAgentMarketRL:environment.get_basic_agent_info}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{environment.}}\sphinxbfcode{\sphinxupquote{get\_basic\_agent\_info}}}{\emph{\DUrole{n}{agent\_dict}}}{}
\end{fulllineitems}

\phantomsection\label{\detokenize{MultiAgentMarketRL:module-markets}}\index{module@\spxentry{module}!markets@\spxentry{markets}}\index{markets@\spxentry{markets}!module@\spxentry{module}}\index{BaseMarketEngine (class in markets)@\spxentry{BaseMarketEngine}\spxextra{class in markets}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{MultiAgentMarketRL:markets.BaseMarketEngine}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{markets.}}\sphinxbfcode{\sphinxupquote{BaseMarketEngine}}}{\emph{\DUrole{n}{n\_sellers}}, \emph{\DUrole{n}{n\_buyers}}, \emph{\DUrole{n}{device}\DUrole{o}{=}\DUrole{default_value}{device(type=\textquotesingle{}cpu\textquotesingle{})}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}~\index{\_\_init\_\_() (markets.BaseMarketEngine method)@\spxentry{\_\_init\_\_()}\spxextra{markets.BaseMarketEngine method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{MultiAgentMarketRL:markets.BaseMarketEngine.__init__}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{\_\_init\_\_}}}{\emph{\DUrole{n}{n\_sellers}}, \emph{\DUrole{n}{n\_buyers}}, \emph{\DUrole{n}{device}\DUrole{o}{=}\DUrole{default_value}{device(type=\textquotesingle{}cpu\textquotesingle{})}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}~\begin{description}
\item[{n\_sellers: int}] \leavevmode
\sphinxAtStartPar
Number of agent sellers

\item[{n\_buyers: int}] \leavevmode
\sphinxAtStartPar
Number of agent buyers

\item[{device: torch.device}] \leavevmode
\sphinxAtStartPar
Allows to allocate the market engine to a cpu or gpu device

\item[{kwargs: optional}] \leavevmode
\sphinxAtStartPar
max\_steps (default=30), max time steps of one episode/game

\end{description}

\end{fulllineitems}

\index{calculate\_deals() (markets.BaseMarketEngine method)@\spxentry{calculate\_deals()}\spxextra{markets.BaseMarketEngine method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{MultiAgentMarketRL:markets.BaseMarketEngine.calculate_deals}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{calculate\_deals}}}{\emph{\DUrole{n}{s\_actions}\DUrole{p}{:} \DUrole{n}{torch.Tensor}}, \emph{\DUrole{n}{b\_actions}\DUrole{p}{:} \DUrole{n}{torch.Tensor}}}{{ $\rightarrow$ Tuple\DUrole{p}{{[}}torch.Tensor\DUrole{p}{, }torch.Tensor\DUrole{p}{{]}}}}
\end{fulllineitems}

\index{reset() (markets.BaseMarketEngine method)@\spxentry{reset()}\spxextra{markets.BaseMarketEngine method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{MultiAgentMarketRL:markets.BaseMarketEngine.reset}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{reset}}}{}{}
\sphinxAtStartPar
Reset the market to its initial unmatched state.

\end{fulllineitems}

\index{step() (markets.BaseMarketEngine method)@\spxentry{step()}\spxextra{markets.BaseMarketEngine method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{MultiAgentMarketRL:markets.BaseMarketEngine.step}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{step}}}{\emph{\DUrole{n}{s\_actions}\DUrole{p}{:} \DUrole{n}{torch.Tensor}}, \emph{\DUrole{n}{b\_actions}\DUrole{p}{:} \DUrole{n}{torch.Tensor}}}{{ $\rightarrow$ Tuple\DUrole{p}{{[}}torch.Tensor\DUrole{p}{, }torch.Tensor\DUrole{p}{{]}}}}
\sphinxAtStartPar
Does a market step appending actions and deals to history and forwarding market time
\begin{description}
\item[{s\_actions}] \leavevmode{[}torch.Tensor{]}
\sphinxAtStartPar
Tensor of seller actions, shape (n\_sellers,)

\item[{b\_actions}] \leavevmode{[}torch.Tensor{]}
\sphinxAtStartPar
Tensor of buyer actions, shape (n\_buyers,)

\end{description}
\begin{description}
\item[{typing.Tuple{[}torch.Tensor torch.Tensor{]}}] \leavevmode
\sphinxAtStartPar
(deals\_sellers, deals\_buyers) with shapes (n\_sellers,), (n\_buyers,)

\end{description}

\end{fulllineitems}


\end{fulllineitems}

\index{MarketMatchHiLo (class in markets)@\spxentry{MarketMatchHiLo}\spxextra{class in markets}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{MultiAgentMarketRL:markets.MarketMatchHiLo}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{markets.}}\sphinxbfcode{\sphinxupquote{MarketMatchHiLo}}}{\emph{\DUrole{n}{n\_sellers}}, \emph{\DUrole{n}{n\_buyers}}, \emph{\DUrole{n}{device}\DUrole{o}{=}\DUrole{default_value}{device(type=\textquotesingle{}cpu\textquotesingle{})}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\sphinxAtStartPar
Market engine using mechanism that highest buying offer is matched with lowest selling offer
\index{\_\_init\_\_() (markets.MarketMatchHiLo method)@\spxentry{\_\_init\_\_()}\spxextra{markets.MarketMatchHiLo method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{MultiAgentMarketRL:markets.MarketMatchHiLo.__init__}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{\_\_init\_\_}}}{\emph{\DUrole{n}{n\_sellers}}, \emph{\DUrole{n}{n\_buyers}}, \emph{\DUrole{n}{device}\DUrole{o}{=}\DUrole{default_value}{device(type=\textquotesingle{}cpu\textquotesingle{})}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}~\begin{description}
\item[{n\_sellers: int}] \leavevmode
\sphinxAtStartPar
Number of agent sellers

\item[{n\_buyers: int}] \leavevmode
\sphinxAtStartPar
Number of agent buyers

\item[{device: torch.device}] \leavevmode
\sphinxAtStartPar
Allows to allocate the market engine to a cpu or gpu device

\item[{kwargs: optional}] \leavevmode
\sphinxAtStartPar
max\_steps (default=30), max time steps of one episode/game

\end{description}

\end{fulllineitems}

\index{calculate\_deals() (markets.MarketMatchHiLo method)@\spxentry{calculate\_deals()}\spxextra{markets.MarketMatchHiLo method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{MultiAgentMarketRL:markets.MarketMatchHiLo.calculate_deals}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{calculate\_deals}}}{\emph{\DUrole{n}{s\_actions}}, \emph{\DUrole{n}{b\_actions}}}{}
\end{fulllineitems}

\index{reset() (markets.MarketMatchHiLo method)@\spxentry{reset()}\spxextra{markets.MarketMatchHiLo method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{MultiAgentMarketRL:markets.MarketMatchHiLo.reset}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{reset}}}{}{}
\sphinxAtStartPar
Reset the market to its initial unmatched state.

\end{fulllineitems}

\index{step() (markets.MarketMatchHiLo method)@\spxentry{step()}\spxextra{markets.MarketMatchHiLo method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{MultiAgentMarketRL:markets.MarketMatchHiLo.step}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{step}}}{\emph{\DUrole{n}{s\_actions}\DUrole{p}{:} \DUrole{n}{torch.Tensor}}, \emph{\DUrole{n}{b\_actions}\DUrole{p}{:} \DUrole{n}{torch.Tensor}}}{{ $\rightarrow$ Tuple\DUrole{p}{{[}}torch.Tensor\DUrole{p}{, }torch.Tensor\DUrole{p}{{]}}}}
\sphinxAtStartPar
Does a market step appending actions and deals to history and forwarding market time
\begin{description}
\item[{s\_actions}] \leavevmode{[}torch.Tensor{]}
\sphinxAtStartPar
Tensor of seller actions, shape (n\_sellers,)

\item[{b\_actions}] \leavevmode{[}torch.Tensor{]}
\sphinxAtStartPar
Tensor of buyer actions, shape (n\_buyers,)

\end{description}
\begin{description}
\item[{typing.Tuple{[}torch.Tensor torch.Tensor{]}}] \leavevmode
\sphinxAtStartPar
(deals\_sellers, deals\_buyers) with shapes (n\_sellers,), (n\_buyers,)

\end{description}

\end{fulllineitems}


\end{fulllineitems}

\phantomsection\label{\detokenize{MultiAgentMarketRL:module-trainer}}\index{module@\spxentry{module}!trainer@\spxentry{trainer}}\index{trainer@\spxentry{trainer}!module@\spxentry{module}}\index{DeepQTrainer (class in trainer)@\spxentry{DeepQTrainer}\spxextra{class in trainer}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{MultiAgentMarketRL:trainer.DeepQTrainer}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{trainer.}}\sphinxbfcode{\sphinxupquote{DeepQTrainer}}}{\emph{\DUrole{n}{env}}, \emph{\DUrole{n}{memory\_size}}, \emph{\DUrole{n}{replay\_start\_size}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}~\index{\_\_init\_\_() (trainer.DeepQTrainer method)@\spxentry{\_\_init\_\_()}\spxextra{trainer.DeepQTrainer method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{MultiAgentMarketRL:trainer.DeepQTrainer.__init__}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{\_\_init\_\_}}}{\emph{\DUrole{n}{env}}, \emph{\DUrole{n}{memory\_size}}, \emph{\DUrole{n}{replay\_start\_size}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}~\begin{description}
\item[{env: Environment object}] \leavevmode
\sphinxAtStartPar
The current environment class object.

\item[{memory\_size: int}] \leavevmode
\sphinxAtStartPar
ReplayBuffer size

\item[{replay\_start\_size: int}] \leavevmode
\sphinxAtStartPar
Number of ReplayBuffer slots to be initialised with a uniform random policy before learning starts

\item[{kwargs: Optional keyword arguments}] \leavevmode\begin{description}
\item[{discount: float, optional (default=0.99)}] \leavevmode
\sphinxAtStartPar
Multiplicative discount factor for Q\sphinxhyphen{}learning update

\item[{update\_frq: int, optional (default=100)}] \leavevmode
\sphinxAtStartPar
Frequency (measured in episode counts) with which the target network is updated

\item[{max\_loss\_history: int, optional (default=None)}] \leavevmode
\sphinxAtStartPar
Number of previous episodes for which the loss will be saved for monitoring
None \textendash{}\textgreater{} All episode losses are saved

\item[{max\_reward\_history: int, optional (default=None)}] \leavevmode
\sphinxAtStartPar
Number of previous episodes for which the rewards will be saved for monitoring
None \textendash{}\textgreater{} All episode rewards are saved

\item[{max\_action\_history: int, optional (default=None)}] \leavevmode
\sphinxAtStartPar
Number of previous episodes for which the actions will be saved for monitoring
None \textendash{}\textgreater{} All episode actions are saved

\item[{loss\_min: int, optional (default=\sphinxhyphen{}5)}] \leavevmode
\sphinxAtStartPar
Lower\sphinxhyphen{}bound for the loss to be clamped to

\item[{loss\_max: int, optional (default=5)}] \leavevmode
\sphinxAtStartPar
Upper\sphinxhyphen{}bound for the loss to be clamped to

\item[{save\_weights: bool, optional (default=False)}] \leavevmode
\sphinxAtStartPar
If true, all agent weights will be saved to the respective directory specified by the agent in question

\end{description}

\end{description}

\end{fulllineitems}

\index{generate\_Q\_targets() (trainer.DeepQTrainer method)@\spxentry{generate\_Q\_targets()}\spxextra{trainer.DeepQTrainer method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{MultiAgentMarketRL:trainer.DeepQTrainer.generate_Q_targets}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{generate\_Q\_targets}}}{\emph{\DUrole{n}{obs\_next}}, \emph{\DUrole{n}{reward}}, \emph{\DUrole{n}{agent\_state}}, \emph{\DUrole{n}{end\_of\_eps}}, \emph{\DUrole{n}{discount}\DUrole{o}{=}\DUrole{default_value}{0.99}}}{}
\sphinxAtStartPar
Generates the Q\sphinxhyphen{}value targets used to update the agent QNetwork
Parameters
———\sphinxhyphen{}
obs\_next: torch.Tensor
\begin{quote}

\sphinxAtStartPar
Tensor containing all observations for sampled time steps t\_j + 1
\end{quote}
\begin{description}
\item[{reward: torch.Tensor}] \leavevmode
\sphinxAtStartPar
Tensor containing all rewards for the sampled time steps t\_j

\item[{agent\_state: torch.Tensor}] \leavevmode
\sphinxAtStartPar
Tensor indicating if an individual agent was finished at sampled time steps t\_j

\item[{end\_of\_eps: ndarray}] \leavevmode
\sphinxAtStartPar
Bool array indicating if episode was finished at sampled time steps t\_j

\item[{discount: float}] \leavevmode
\sphinxAtStartPar
Discount factor gamma used in the Q\sphinxhyphen{}learning update.

\end{description}
\begin{description}
\item[{targets: torch.Tensor}] \leavevmode
\sphinxAtStartPar
Tensor containing all targets y\_j used to perform a gradient descent step on (y\_j \sphinxhyphen{} Q(s\_j, a\_j; theta))**2
Tensor will have shape (batch\_size, n\_agents)
targets{[}:, :n\_sellers{]} contains all targets for the seller agents
targets{[}:, n\_sellers:{]} contains all targets for the buyer agents

\end{description}

\end{fulllineitems}

\index{generate\_Q\_values() (trainer.DeepQTrainer method)@\spxentry{generate\_Q\_values()}\spxextra{trainer.DeepQTrainer method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{MultiAgentMarketRL:trainer.DeepQTrainer.generate_Q_values}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{generate\_Q\_values}}}{\emph{\DUrole{n}{obs}}, \emph{\DUrole{n}{act}}}{}
\sphinxAtStartPar
Generates the Q\sphinxhyphen{}values for each agent
\begin{description}
\item[{obs: torch.tensor}] \leavevmode
\sphinxAtStartPar
All current observations

\item[{act: torch.tensor}] \leavevmode
\sphinxAtStartPar
All current actions

\end{description}
\begin{description}
\item[{q\_values: torch.tensor}] \leavevmode
\sphinxAtStartPar
The Q\sphinxhyphen{}values of the individual agents

\end{description}

\end{fulllineitems}

\index{get\_agent\_Q\_target() (trainer.DeepQTrainer static method)@\spxentry{get\_agent\_Q\_target()}\spxextra{trainer.DeepQTrainer static method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{MultiAgentMarketRL:trainer.DeepQTrainer.get_agent_Q_target}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{static }}\sphinxbfcode{\sphinxupquote{get\_agent\_Q\_target}}}{\emph{\DUrole{n}{agent}}, \emph{\DUrole{n}{observations}}, \emph{\DUrole{n}{agent\_state}}}{}
\sphinxAtStartPar
Returns all the Q\sphinxhyphen{}targets of the different agents

\sphinxAtStartPar
agent: agent class instance
observations: torch.tensor
\begin{quote}

\sphinxAtStartPar
All current observations
\end{quote}
\begin{description}
\item[{agent\_state: torch.tensor}] \leavevmode
\sphinxAtStartPar
The state (active/finished) of all agents

\end{description}
\begin{description}
\item[{targets: torch.tensor}] \leavevmode
\sphinxAtStartPar
Q\sphinxhyphen{}targets of all agents

\end{description}

\end{fulllineitems}

\index{get\_agent\_Q\_values() (trainer.DeepQTrainer static method)@\spxentry{get\_agent\_Q\_values()}\spxextra{trainer.DeepQTrainer static method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{MultiAgentMarketRL:trainer.DeepQTrainer.get_agent_Q_values}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{static }}\sphinxbfcode{\sphinxupquote{get\_agent\_Q\_values}}}{\emph{\DUrole{n}{agent}}, \emph{\DUrole{n}{observations}}, \emph{\DUrole{n}{actions}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
\sphinxAtStartPar
Returns all the Q\sphinxhyphen{}values of the different agents

\sphinxAtStartPar
agent: agent class instance
observations: torch.tensor
\begin{quote}

\sphinxAtStartPar
All current observations
\end{quote}
\begin{description}
\item[{actions: torch.tensor}] \leavevmode
\sphinxAtStartPar
All current actions

\end{description}
\begin{description}
\item[{q\_values: torch.tensor}] \leavevmode
\sphinxAtStartPar
Q\sphinxhyphen{}values of all agents

\end{description}

\end{fulllineitems}

\index{mse\_loss() (trainer.DeepQTrainer method)@\spxentry{mse\_loss()}\spxextra{trainer.DeepQTrainer method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{MultiAgentMarketRL:trainer.DeepQTrainer.mse_loss}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{mse\_loss}}}{\emph{\DUrole{n}{q\_targets}}, \emph{\DUrole{n}{q\_values}}}{}
\sphinxAtStartPar
Custom MSE\sphinxhyphen{}loss with clamping
\begin{description}
\item[{q\_targets: torch.tensor}] \leavevmode
\sphinxAtStartPar
All the Q\sphinxhyphen{}value targets

\item[{q\_values: torch.tensor}] \leavevmode
\sphinxAtStartPar
All the Q\sphinxhyphen{}values chosen by the agents

\end{description}
\begin{description}
\item[{loss: torch.tensor}] \leavevmode
\sphinxAtStartPar
The clamped mean squared error loss

\end{description}

\end{fulllineitems}

\index{set\_replay\_buffer() (trainer.DeepQTrainer method)@\spxentry{set\_replay\_buffer()}\spxextra{trainer.DeepQTrainer method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{MultiAgentMarketRL:trainer.DeepQTrainer.set_replay_buffer}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{set\_replay\_buffer}}}{\emph{\DUrole{n}{memory\_size}}, \emph{\DUrole{n}{replay\_start\_size}}}{}
\sphinxAtStartPar
Initializes the first N replay buffer entries with random actions.
Parameters
———\sphinxhyphen{}
memory\_size: int
\begin{quote}

\sphinxAtStartPar
Total size of the replay buffer
\end{quote}
\begin{description}
\item[{replay\_start\_size: int}] \leavevmode
\sphinxAtStartPar
Number of buffer entries to be initialized with random actions (corresponds to the integer N)

\end{description}
\begin{description}
\item[{buffer: tianshou.data.ReplayBuffer}] \leavevmode
\sphinxAtStartPar
Initialised replay buffer

\end{description}

\end{fulllineitems}

\index{train() (trainer.DeepQTrainer method)@\spxentry{train()}\spxextra{trainer.DeepQTrainer method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{MultiAgentMarketRL:trainer.DeepQTrainer.train}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{train}}}{\emph{\DUrole{n}{n\_episodes}}, \emph{\DUrole{n}{batch\_size}}}{}
\sphinxAtStartPar
Training method.
\begin{description}
\item[{n\_episodes: int}] \leavevmode
\sphinxAtStartPar
Number of episodes (games) to train for

\item[{batch\_size: int}] \leavevmode
\sphinxAtStartPar
Batch size used to update the agents network weights

\end{description}
\begin{description}
\item[{list}] \leavevmode
\sphinxAtStartPar
list{[}0{]}: Average loss history
list{[}1{]}: Average reward history
list{[}2{]}: Action history

\end{description}

\end{fulllineitems}


\end{fulllineitems}



\renewcommand{\indexname}{Python Module Index}
\begin{sphinxtheindex}
\let\bigletter\sphinxstyleindexlettergroup
\bigletter{a}
\item\relax\sphinxstyleindexentry{agents}\sphinxstyleindexpageref{MultiAgentMarketRL:\detokenize{module-agents}}
\indexspace
\bigletter{e}
\item\relax\sphinxstyleindexentry{environment}\sphinxstyleindexpageref{MultiAgentMarketRL:\detokenize{module-environment}}
\item\relax\sphinxstyleindexentry{exploration\_setting}\sphinxstyleindexpageref{MultiAgentMarketRL:\detokenize{module-exploration_setting}}
\indexspace
\bigletter{i}
\item\relax\sphinxstyleindexentry{info\_setting}\sphinxstyleindexpageref{MultiAgentMarketRL:\detokenize{module-info_setting}}
\indexspace
\bigletter{m}
\item\relax\sphinxstyleindexentry{markets}\sphinxstyleindexpageref{MultiAgentMarketRL:\detokenize{module-markets}}
\indexspace
\bigletter{n}
\item\relax\sphinxstyleindexentry{network\_models}\sphinxstyleindexpageref{MultiAgentMarketRL:\detokenize{module-network_models}}
\indexspace
\bigletter{r}
\item\relax\sphinxstyleindexentry{reward\_setting}\sphinxstyleindexpageref{MultiAgentMarketRL:\detokenize{module-reward_setting}}
\indexspace
\bigletter{t}
\item\relax\sphinxstyleindexentry{trainer}\sphinxstyleindexpageref{MultiAgentMarketRL:\detokenize{module-trainer}}
\end{sphinxtheindex}

\renewcommand{\indexname}{Index}
\printindex
\end{document}